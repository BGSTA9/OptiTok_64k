{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c001b199",
   "metadata": {},
   "source": [
    "what is the overall responsibility of the following Python code And Why is it important to be employed? An ultimate one-liner summary for the overall block of code. And I need for each function to know what it does and why is it matter?\n",
    "\n",
    "'''python\n",
    "\n",
    "\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fd2ba2f",
   "metadata": {},
   "source": [
    "# **tiktoken/<span style='color:orange'> **tests** </span>**\n",
    "\n",
    "- _ init _.py\n",
    "- test_encoding.py\n",
    "- test_helpers.py\n",
    "- test_misc.py\n",
    "- test_offsets.py\n",
    "- test_pickle.py\n",
    "- test_simple_public.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d10ec80c",
   "metadata": {},
   "source": [
    "## **tiktoken/tests/<span style='color:orange'> **test_encoding.py** </span>**\n",
    "\n",
    "**Why these tests matter (intent & responsibilities)**<br>\n",
    "1. Correctness ‚Äî ensure encode & decode are inverses where appropriate (ordinary text) and consistent for single tokens.<br>\n",
    "2.\tUnicode safety ‚Äî correct behavior for emojis, surrogate pairs, and malformed Unicode.<br>\n",
    "3.\tBinary handling ‚Äî stable encoding/decoding of arbitrary bytes.<br>\n",
    "4.\tSpecial-token safety ‚Äî correct and safe handling of reserved tokens (prevent accidental injection of control tokens).<br>\n",
    "5.\tRobustness & limits ‚Äî tests for very large or repetitive inputs and ensuring the library behaves deterministically or fails safely.<br>\n",
    "6.\tRegression protection ‚Äî many of these tests capture expected token ids (deterministic assertions) so future changes that break token assignments or regex tokenization rules get flagged.<br>\n",
    "7.\tAPI contract ‚Äî batch APIs, allowed/disallowed flags, encode_ordinary behavior ‚Äî the test suite documents and enforces the intended API semantics.\n",
    "\n",
    "- @pytest.mark.parametrize(\"make_enc\", ENCODING_FACTORIES) runs the same test for each encoding factory, so the library is tested across several encoder configurations.\n",
    "- hypothesis decorators (@given, @settings) produce many random examples to catch edge cases not covered by fixed examples. deadline=None disables per-test timeout; max_examples=MAX_EXAMPLES controls number of tries.\n",
    "- with pytest.raises(ValueError): ensures specific failures happen for disallowed conditions.\n",
    "- assert statements validate expectations ‚Äî if any fail, pytest reports the failing test.\n",
    "\n",
    "**Examples of bugs these tests could catch**\n",
    "\n",
    "- A change in token-to-string mapping that makes encode(\"hello world\") yield different tokens ‚Üí test_simple fails.\n",
    "- A Unicode bug that mishandles surrogate pairs ‚Üí test_encode_surrogate_pairs fails.\n",
    "- An unchecked special token injection that should raise ‚Üí test_special_token exposes it.\n",
    "- A buffer overflow / OOM for huge inputs ‚Äî ValueError not raised or memory blow-up ‚Üí test_large_repeated fails or reveals slow behavior.\n",
    "- Batch encode returning wrong order or mutated inputs ‚Üí test_batch_encode or Hypothesis batch tests fail.\n",
    "\n",
    "**Summary (one-liner)**\n",
    "\n",
    "- This file is a comprehensive unit + property-test suite that verifies correctness, robustness, Unicode/bytes safety, special-token semantics, and batch behavior of the tiktoken encoding implementation across multiple tokenizer configurations.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de859a28",
   "metadata": {},
   "source": [
    "Smoke test for the most basic encoding/decoding functionality across known encodings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "47e8ac24",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_simple():\n",
    "    enc = tiktoken.get_encoding(\"gpt2\")\n",
    "    assert enc.encode(\"hello world\") == [31373, 995]\n",
    "    assert enc.decode([31373, 995]) == \"hello world\"\n",
    "    assert enc.encode(\"hello <|endoftext|>\", allowed_special=\"all\") == [31373, 220, 50256]\n",
    "\n",
    "    enc = tiktoken.get_encoding(\"cl100k_base\")\n",
    "    assert enc.encode(\"hello world\") == [15339, 1917]\n",
    "    assert enc.decode([15339, 1917]) == \"hello world\"\n",
    "    assert enc.encode(\"hello <|endoftext|>\", allowed_special=\"all\") == [15339, 220, 100257]\n",
    "\n",
    "    for enc_name in tiktoken.list_encoding_names():\n",
    "        enc = tiktoken.get_encoding(enc_name)\n",
    "        for token in range(min(10_000, enc.max_token_value - 1)):\n",
    "            assert enc.encode_single_token(enc.decode_single_token_bytes(token)) == token"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6ce59d2",
   "metadata": {},
   "source": [
    "Check how the tokenizer handles repeated identical characters (here \"0\" repeated many times). This catches regressions in tokenization logic for repetitive sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ffddee98",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_simple_repeated():\n",
    "    enc = tiktoken.get_encoding(\"gpt2\")\n",
    "    assert enc.encode(\"0\") == [15]\n",
    "    assert enc.encode(\"00\") == [405]\n",
    "    assert enc.encode(\"000\") == [830]\n",
    "    assert enc.encode(\"0000\") == [2388]\n",
    "    assert enc.encode(\"00000\") == [20483]\n",
    "    assert enc.encode(\"000000\") == [10535]\n",
    "    assert enc.encode(\"0000000\") == [24598]\n",
    "    assert enc.encode(\"00000000\") == [8269]\n",
    "    assert enc.encode(\"000000000\") == [10535, 830]\n",
    "    assert enc.encode(\"0000000000\") == [8269, 405]\n",
    "    assert enc.encode(\"00000000000\") == [8269, 830]\n",
    "    assert enc.encode(\"000000000000\") == [8269, 2388]\n",
    "    assert enc.encode(\"0000000000000\") == [8269, 20483]\n",
    "    assert enc.encode(\"00000000000000\") == [8269, 10535]\n",
    "    assert enc.encode(\"000000000000000\") == [8269, 24598]\n",
    "    assert enc.encode(\"0000000000000000\") == [25645]\n",
    "    assert enc.encode(\"00000000000000000\") == [8269, 10535, 830]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77b0f467",
   "metadata": {},
   "source": [
    "Ensure extremely long inputs are either handled safely or rejected. It fetches an encoding and asserts that encoding a million-character string raises ValueError. Tests boundary behavior / limits / defensive checks (prevents OOM [Out Of Memory] or runaway behavior)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dd856cee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_large_repeated():\n",
    "    enc = tiktoken.get_encoding(\"o200k_base\")\n",
    "\n",
    "    with pytest.raises(ValueError):\n",
    "        enc.encode(\"x\" * 1_000_000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56130b59",
   "metadata": {},
   "source": [
    "Check tokenizer behavior for a few tricky strings with punctuation, newlines, whitespace patterns. Ensures tokenization rules (which often use regexes) are producing consistent token ids."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50a06283",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_simple_regex():\n",
    "    enc = tiktoken.get_encoding(\"cl100k_base\")\n",
    "    assert enc.encode(\"rer\") == [38149]\n",
    "    assert enc.encode(\"'rer\") == [2351, 81]\n",
    "    assert enc.encode(\"today\\n \") == [31213, 198, 220]\n",
    "    assert enc.encode(\"today\\n \\n\") == [31213, 27907]\n",
    "    assert enc.encode(\"today\\n  \\n\") == [31213, 14211]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e745367a",
   "metadata": {},
   "source": [
    "Test basic encode behavior for several prebuilt encodings; check that certain byte sequences map to expected tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a8cb9e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_basic_encode():\n",
    "    enc = tiktoken.get_encoding(\"r50k_base\") # >>> r <<< 50k_base\n",
    "    assert enc.encode(\"hello world\") == [31373, 995]\n",
    "\n",
    "    enc = tiktoken.get_encoding(\"p50k_base\") # >>> p <<< 50k_base\n",
    "    assert enc.encode(\"hello world\") == [31373, 995]\n",
    "\n",
    "    enc = tiktoken.get_encoding(\"cl100k_base\")\n",
    "    assert enc.encode(\"hello world\") == [15339, 1917]\n",
    "    assert enc.encode(\" \\x850\") == [220, 126, 227, 15]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30b6b329",
   "metadata": {},
   "source": [
    "Encoding an empty string should return empty list []."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea4f69e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_encode_empty():\n",
    "    enc = tiktoken.get_encoding(\"r50k_base\")\n",
    "    assert enc.encode(\"\") == []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "025eab55",
   "metadata": {},
   "source": [
    "test lower-level bytes-encoding functions. The Hypothesis-powered test_hyp_encode_bytes uses @given(bytestring=st.binary()) to check for many random byte sequences that _encode_bytes ‚Üí decode_bytes is an actual roundtrip. This helps find corner cases in binary handling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ff1c3dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "@pytest.mark.parametrize(\"make_enc\", ENCODING_FACTORIES)\n",
    "@hypothesis.given(bytestring=st.binary())\n",
    "@hypothesis.settings(deadline=None, max_examples=MAX_EXAMPLES)\n",
    "\n",
    "def test_encode_bytes():\n",
    "    enc = tiktoken.get_encoding(\"cl100k_base\")\n",
    "    assert enc._encode_bytes(b\" \\xec\\x8b\\xa4\\xed\") == [62085]\n",
    "    for i in range(10):\n",
    "        bytestring = b\"\\x80\" * i\n",
    "        assert enc.decode_bytes(enc._encode_bytes(bytestring)) == bytestring\n",
    "\n",
    "def test_hyp_encode_bytes(make_enc: Callable[[], tiktoken.Encoding], bytestring: bytes):\n",
    "    enc = make_enc()\n",
    "    assert enc.decode_bytes(enc._encode_bytes(bytestring)) == bytestring"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abe9394c",
   "metadata": {},
   "source": [
    "Unicode correctness. Emoji and other characters are represented as codepoints or surrogate pairs in UTF-16:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e45fd1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_encode_surrogate_pairs():\n",
    "    enc = tiktoken.get_encoding(\"cl100k_base\")\n",
    "\n",
    "    assert enc.encode(\"üëç\") == [9468, 239, 235]\n",
    "    # surrogate pair gets converted to codepoint\n",
    "    assert enc.encode(\"\\ud83d\\udc4d\") == [9468, 239, 235]\n",
    "\n",
    "    # lone surrogate just gets replaced\n",
    "    assert enc.encode(\"\\ud83d\") == enc.encode(\"ÔøΩ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94ed188c",
   "metadata": {},
   "source": [
    "more robustness tests for huge repetitive strings of different characters. Ensures encode/decode roundtrip for large strings and slight variations (leading space, trailing newline). Checks the encoder can handle long repetitive input without corrupting it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c377bfbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "@pytest.mark.parametrize(\"make_enc\", ENCODING_FACTORIES)\n",
    "\n",
    "def test_catastrophically_repetitive(make_enc: Callable[[], tiktoken.Encoding]):\n",
    "    enc = make_enc()\n",
    "    for c in [\"^\", \"0\", \"a\", \"'s\", \" \", \"\\n\"]:\n",
    "        big_value = c * 10_000\n",
    "        assert big_value == enc.decode(enc.encode(big_value))\n",
    "\n",
    "        big_value = \" \" + big_value\n",
    "        assert big_value == enc.decode(enc.encode(big_value))\n",
    "\n",
    "        big_value = big_value + \"\\n\"\n",
    "        assert big_value == enc.decode(enc.encode(big_value))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d164e9b",
   "metadata": {},
   "source": [
    "**Roundtrip tests**\n",
    "\n",
    "*These tests validate that encode followed by decode returns the original input.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c65b346",
   "metadata": {},
   "source": [
    "For a fixed list of strings (including non-ASCII \"ËØ∑ËÄÉËØïÊàëÁöÑËΩØ‰ª∂ÔºÅ12345\"), confirms enc.decode(enc.encode(value)) == value. Also checks encode_ordinary (which probably encodes ignoring special tokens) behaves same for ordinary text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f2ef824",
   "metadata": {},
   "outputs": [],
   "source": [
    "@pytest.mark.parametrize(\"make_enc\", ENCODING_FACTORIES)\n",
    "def test_basic_roundtrip(make_enc):\n",
    "    enc = make_enc()\n",
    "    for value in (\n",
    "        \"hello\",\n",
    "        \"hello \",\n",
    "        \"hello  \",\n",
    "        \" hello\",\n",
    "        \" hello \",\n",
    "        \" hello  \",\n",
    "        \"hello world\",\n",
    "        \"ËØ∑ËÄÉËØïÊàëÁöÑËΩØ‰ª∂ÔºÅ12345\",\n",
    "    ):\n",
    "        assert value == enc.decode(enc.encode(value))\n",
    "        assert value == enc.decode(enc.encode_ordinary(value))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94cc4da6",
   "metadata": {},
   "source": [
    "Hypothesis-based: @given(text=st.text()) generates many random Unicode strings; confirms encode->decode roundtrip property across lots of inputs and for every encoding factory. This is a powerful property test to catch many edge cases.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a41e5341",
   "metadata": {},
   "outputs": [],
   "source": [
    "@pytest.mark.parametrize(\"make_enc\", ENCODING_FACTORIES)\n",
    "@hypothesis.given(text=st.text())\n",
    "@hypothesis.settings(deadline=None, max_examples=MAX_EXAMPLES)\n",
    "\n",
    "def test_hyp_roundtrip(make_enc: Callable[[], tiktoken.Encoding], text):\n",
    "    enc = make_enc()\n",
    "    assert text == enc.decode(enc.encode(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dca6b27",
   "metadata": {},
   "source": [
    "For every token id in enc.n_vocab:\n",
    "\t‚Ä¢\tTry to get token_bytes = enc.decode_single_token_bytes(token) (some tokens may raise KeyError if invalid)\n",
    "\t‚Ä¢\tAssert that enc.encode_single_token(token_bytes) == token.\n",
    "‚Üí Ensures individual token encode/decode pair is consistent across vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9f25147",
   "metadata": {},
   "outputs": [],
   "source": [
    "@pytest.mark.parametrize(\"make_enc\", ENCODING_FACTORIES)\n",
    "\n",
    "def test_single_token_roundtrip(make_enc: Callable[[], tiktoken.Encoding]):\n",
    "    enc = make_enc()\n",
    "\n",
    "    for token in range(enc.n_vocab):\n",
    "        try:\n",
    "            token_bytes = enc.decode_single_token_bytes(token)\n",
    "        except KeyError:\n",
    "            continue\n",
    "        assert enc.encode_single_token(token_bytes) == token"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8bc4151",
   "metadata": {},
   "source": [
    "**Special tokens tests**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65fb32c1",
   "metadata": {},
   "source": [
    "This whole block ensures strict and predictable API semantics around special tokens, which is crucial when special tokens carry model control semantics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d5c105d",
   "metadata": {},
   "outputs": [],
   "source": [
    "@pytest.mark.parametrize(\"make_enc\", ENCODING_FACTORIES)\n",
    "@hypothesis.given(text=st.text())\n",
    "@hypothesis.settings(deadline=None, max_examples=MAX_EXAMPLES)\n",
    "\n",
    "def test_special_token():\n",
    "    enc = tiktoken.get_encoding(\"cl100k_base\")\n",
    "\n",
    "    eot = enc.encode_single_token(\"<|endoftext|>\")\n",
    "    assert eot == enc.eot_token\n",
    "    fip = enc.encode_single_token(\"<|fim_prefix|>\")\n",
    "    fim = enc.encode_single_token(\"<|fim_middle|>\")\n",
    "\n",
    "    text = \"<|endoftext|> hello <|fim_prefix|>\"\n",
    "    assert eot not in enc.encode(text, disallowed_special=())\n",
    "    with pytest.raises(ValueError):\n",
    "        enc.encode(text)\n",
    "    with pytest.raises(ValueError):\n",
    "        enc.encode(text, disallowed_special=\"all\")\n",
    "    with pytest.raises(ValueError):\n",
    "        enc.encode(text, disallowed_special={\"<|endoftext|>\"})\n",
    "    with pytest.raises(ValueError):\n",
    "        enc.encode(text, disallowed_special={\"<|fim_prefix|>\"})\n",
    "\n",
    "    text = \"<|endoftext|> hello <|fim_prefix|> there <|fim_middle|>\"\n",
    "    tokens = enc.encode(text, disallowed_special=())\n",
    "    assert eot not in tokens\n",
    "    assert fip not in tokens\n",
    "    assert fim not in tokens\n",
    "\n",
    "    tokens = enc.encode(text, allowed_special=\"all\", disallowed_special=())\n",
    "    assert eot in tokens\n",
    "    assert fip in tokens\n",
    "    assert fim in tokens\n",
    "\n",
    "    tokens = enc.encode(text, allowed_special=\"all\", disallowed_special=\"all\")\n",
    "    assert eot in tokens\n",
    "    assert fip in tokens\n",
    "    assert fim in tokens\n",
    "\n",
    "    tokens = enc.encode(text, allowed_special={\"<|fim_prefix|>\"}, disallowed_special=())\n",
    "    assert eot not in tokens\n",
    "    assert fip in tokens\n",
    "    assert fim not in tokens\n",
    "\n",
    "    tokens = enc.encode(text, allowed_special={\"<|endoftext|>\"}, disallowed_special=())\n",
    "    assert eot in tokens\n",
    "    assert fip not in tokens\n",
    "    assert fim not in tokens\n",
    "\n",
    "    tokens = enc.encode(text, allowed_special={\"<|fim_middle|>\"}, disallowed_special=())\n",
    "    assert eot not in tokens\n",
    "    assert fip not in tokens\n",
    "    assert fim in tokens\n",
    "    \n",
    "def test_hyp_special_ordinary(make_enc, text: str):\n",
    "    enc = make_enc()\n",
    "    assert enc.encode_ordinary(text) == enc.encode(text, disallowed_special=())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdb9b820",
   "metadata": {},
   "source": [
    "**Batch encoding tests**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10964a73",
   "metadata": {},
   "source": [
    "The Hypothesis batch test generates lists of random strings and checks encode_batch/decode_batch roundtrip for whole batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54dd0c96",
   "metadata": {},
   "outputs": [],
   "source": [
    "@pytest.mark.parametrize(\"make_enc\", ENCODING_FACTORIES)\n",
    "\n",
    "def test_batch_encode(make_enc: Callable[[], tiktoken.Encoding]):\n",
    "    enc = make_enc()\n",
    "    text1 = \"hello world\"\n",
    "    text2 = \"goodbye world\"\n",
    "\n",
    "    assert enc.encode_batch([text1]) == [enc.encode(text1)]\n",
    "    assert enc.encode_batch([text1, text2]) == [enc.encode(text1), enc.encode(text2)]\n",
    "\n",
    "    assert enc.encode_ordinary_batch([text1]) == [enc.encode_ordinary(text1)]\n",
    "    assert enc.encode_ordinary_batch([text1, text2]) == [\n",
    "        enc.encode_ordinary(text1),\n",
    "        enc.encode_ordinary(text2),\n",
    "    ]\n",
    "\n",
    "@pytest.mark.parametrize(\"make_enc\", ENCODING_FACTORIES)\n",
    "@hypothesis.given(batch=st.lists(st.text()))\n",
    "@hypothesis.settings(deadline=None, max_examples=MAX_EXAMPLES)\n",
    "\n",
    "def test_hyp_batch_roundtrip(make_enc: Callable[[], tiktoken.Encoding], batch):\n",
    "    enc = make_enc()\n",
    "\n",
    "    encoded = enc.encode_batch(batch, allowed_special=\"all\")\n",
    "    assert encoded == [enc.encode(t, allowed_special=\"all\") for t in batch]\n",
    "    decoded = enc.decode_batch(encoded)\n",
    "    assert decoded == batch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "462294d1",
   "metadata": {},
   "source": [
    "## **tiktoken/tests/<span style='color:orange'> **test_helpers.py** </span>**\n",
    "\n",
    "This code sets up configurable, named, lazy factories for multiple tokenizer encodings (and a configurable test-size limit) so pytest tests can run the same test logic cleanly and repeatedly across different encodings with clear test IDs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "505d464e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import bisect\n",
    "import functools\n",
    "import os\n",
    "\n",
    "import pytest\n",
    "\n",
    "import tiktoken\n",
    "\n",
    "MAX_EXAMPLES: int = int(os.environ.get(\"TIKTOKEN_MAX_EXAMPLES\", \"100\"))\n",
    "\n",
    "ENCODINGS = [\"r50k_base\", \"cl100k_base\"]\n",
    "SOME_ENCODINGS = [\"cl100k_base\"]\n",
    "\n",
    "\n",
    "ENCODING_FACTORIES = [\n",
    "    pytest.param(functools.partial(tiktoken.get_encoding, name), id=name) for name in ENCODINGS\n",
    "]\n",
    "SOME_ENCODING_FACTORIES = [\n",
    "    pytest.param(functools.partial(tiktoken.get_encoding, name), id=name) for name in SOME_ENCODINGS\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a19fdae6",
   "metadata": {},
   "source": [
    "## **tiktoken/tests/<span style='color:orange'> **test_misc.py** </span>**\n",
    "\n",
    "**one-liner summary:** <br><br>\n",
    "These tests ensure that tiktoken resolves correct model-to-encoding mappings and stays lightweight by not importing optional dependencies unless explicitly needed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9fcc4b2",
   "metadata": {},
   "source": [
    "Verifies that each OpenAI model name correctly maps to its intended tokenizer encoding ‚Äî preventing regressions in token counting behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e22b7901",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import sys\n",
    "\n",
    "import tiktoken\n",
    "\n",
    "\n",
    "def test_encoding_for_model():\n",
    "    enc = tiktoken.encoding_for_model(\"gpt2\")\n",
    "    assert enc.name == \"gpt2\"\n",
    "    enc = tiktoken.encoding_for_model(\"text-davinci-003\")\n",
    "    assert enc.name == \"p50k_base\"\n",
    "    enc = tiktoken.encoding_for_model(\"text-davinci-edit-001\")\n",
    "    assert enc.name == \"p50k_edit\"\n",
    "    enc = tiktoken.encoding_for_model(\"gpt-3.5-turbo-0301\")\n",
    "    assert enc.name == \"cl100k_base\"\n",
    "    enc = tiktoken.encoding_for_model(\"gpt-4\")\n",
    "    assert enc.name == \"cl100k_base\"\n",
    "    enc = tiktoken.encoding_for_model(\"gpt-4o\")\n",
    "    assert enc.name == \"o200k_base\"\n",
    "    enc = tiktoken.encoding_for_model(\"gpt-oss-120b\")\n",
    "    assert enc.name == \"o200k_harmony\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3c22c6b",
   "metadata": {},
   "source": [
    "Ensures tiktoken does not automatically import optional heavy dependencies like blobfile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "863f1d67",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_optional_blobfile_dependency():\n",
    "    prog = \"\"\"\n",
    "import tiktoken\n",
    "import sys\n",
    "assert \"blobfile\" not in sys.modules\n",
    "\"\"\"\n",
    "    subprocess.check_call([sys.executable, \"-c\", prog])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "082ecdf9",
   "metadata": {},
   "source": [
    "## **tiktoken/tests/<span style='color:orange'> **test_offsets.py** </span>**\n",
    "\n",
    "**one-liner summary:** <br><br>\n",
    "This file validates, by a mixture of property-based and hand-written tests, that tiktoken.Encoding.decode_with_offsets returns both the exact decoded text and the correct per-token character start offsets ‚Äî crucial for mapping tokens back to text across languages, special tokens, and tricky UTF-8 boundaries."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e551646",
   "metadata": {},
   "source": [
    "\n",
    "**What the function does** <br>\n",
    "- Returns the number of leading characters that a and b share (length of their common prefix).\n",
    "\n",
    "**Why this matters in the test** <br>\n",
    "- This function is the basic comparison primitive used to compute an offset by comparing a full decoded text to a prefix-decoded text; it tells you how many characters the prefix covers in the full text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e22bd0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable\n",
    "\n",
    "import hypothesis\n",
    "import pytest\n",
    "from hypothesis import strategies as st\n",
    "\n",
    "import tiktoken\n",
    "\n",
    "from .test_helpers import MAX_EXAMPLES, SOME_ENCODING_FACTORIES\n",
    "\n",
    "\n",
    "def _common_prefix_len(a, b):\n",
    "    i = 0\n",
    "    while i < len(a) and i < len(b) and a[i] == b[i]:\n",
    "        i += 1\n",
    "    return i"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e836706b",
   "metadata": {},
   "source": [
    "\n",
    "**What the function does** <br>\n",
    "- Constructs a reference list of character offsets for each token in tokens by decoding the token sequence and computing where each token starts in the decoded string using the naive prefix method.\n",
    "\n",
    "**Why this matters in the test** <br>\n",
    "- This function provides the ground-truth / reference offsets to compare against enc.decode_with_offsets. Because it‚Äôs simple and straightforward (decode prefixes and measure), it‚Äôs unlikely to have the same subtle bugs that an optimized library implementation might have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff640e24",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _token_offsets_reference(enc, tokens):\n",
    "    text = enc.decode(tokens, errors=\"strict\")\n",
    "    res = []\n",
    "    for i in range(len(tokens)):\n",
    "        prefix = enc.decode(tokens[:i], errors=\"ignore\")\n",
    "        res.append(_common_prefix_len(text, prefix))\n",
    "    return res\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc8d2073",
   "metadata": {},
   "source": [
    "**What the function does** <br>\n",
    "- Property-based test that compares the offsets produced by enc.decode_with_offsets(tokens) to the reference offsets produced by _token_offsets_reference(enc, tokens) for many randomly generated valid token sequences and across multiple encoding factories.\n",
    "\n",
    "**Why this matters in the test** <br>\n",
    "- Provides broad coverage and confidence that decode_with_offsets matches a straightforward, correct algorithm across many encodings and input shapes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73e4c867",
   "metadata": {},
   "outputs": [],
   "source": [
    "@pytest.mark.parametrize(\"make_enc\", SOME_ENCODING_FACTORIES)\n",
    "@hypothesis.given(data=st.data())\n",
    "@hypothesis.settings(deadline=None, max_examples=MAX_EXAMPLES)\n",
    "\n",
    "def test_hyp_offsets(make_enc: Callable[[], tiktoken.Encoding], data):\n",
    "    enc = make_enc()\n",
    "\n",
    "    tokens_st = st.lists(\n",
    "        st.integers(0, enc.n_vocab - 1).filter(\n",
    "            lambda x: x in enc._special_tokens.values() or x in enc._mergeable_ranks.values()\n",
    "        ),\n",
    "        min_size=1,\n",
    "        max_size=20,\n",
    "    )\n",
    "    tokens = data.draw(tokens_st)\n",
    "\n",
    "    # This is a dumb hack to make sure that our tokens are a valid UTF-8 string\n",
    "    # We could potentially drop this, see the TODO in decode_with_offsets\n",
    "    tokens = enc.encode(enc.decode(tokens, errors=\"ignore\"), allowed_special=\"all\")\n",
    "    assert enc.decode_with_offsets(tokens)[1] == _token_offsets_reference(enc, tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6edf551f",
   "metadata": {},
   "source": [
    "**What the function does** <br>\n",
    "- A set of deterministic unit tests using explicit strings to check enc.decode_with_offsets in a few specific, important scenarios (simple ascii, special tokens, Chinese, Tamil, examples with tricky UTF-8 byte sequences).\n",
    "\n",
    "**Why this matters in the test** <br>\n",
    "- Provides quick, readable confirmation of correctness for known tricky cases; easier to debug than a failed Hypothesis case without context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "340a9152",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_basic_offsets():\n",
    "    enc = tiktoken.get_encoding(\"cl100k_base\")\n",
    "\n",
    "    prompt = \"hello world\"\n",
    "    p, o = enc.decode_with_offsets(enc.encode(prompt))\n",
    "    assert p == prompt\n",
    "    assert o == [0, 5]\n",
    "\n",
    "    prompt = \"hello world<|endoftext|> green cow\"\n",
    "    p, o = enc.decode_with_offsets(enc.encode(prompt, allowed_special=\"all\"))\n",
    "    assert p == prompt\n",
    "    assert o == [0, 5, 11, 24, 30]\n",
    "\n",
    "    prompt = \"ÊàëÈùûÂ∏∏Ê∏¥Êúõ‰∏é‰∫∫Â∑•Êô∫ËÉΩ‰∏ÄËµ∑Â∑•‰Ωú\"\n",
    "    p, o = enc.decode_with_offsets(enc.encode(prompt))\n",
    "    assert p == prompt\n",
    "    assert o == [0, 1, 2, 3, 3, 4, 4, 5, 6, 7, 8, 8, 9, 10, 11, 12, 13]\n",
    "\n",
    "    # contains the interesting tokens b'\\xe0\\xae\\xbf\\xe0\\xae' and b'\\xe0\\xaf\\x8d\\xe0\\xae'\n",
    "    # in which \\xe0 is the start of a 3-byte UTF-8 character\n",
    "    prompt = \"‡Æ®‡Æü‡Æø‡Æï‡Æ∞‡Øç ‡Æö‡ØÇ‡Æ∞‡Øç‡ÆØ‡Ææ\"\n",
    "    p, o = enc.decode_with_offsets(enc.encode(prompt))\n",
    "    assert p == prompt\n",
    "    assert o == [0, 0, 1, 1, 2, 3, 4, 4, 5, 6, 7, 8, 8, 9, 9, 10, 11, 12, 12]\n",
    "\n",
    "    # contains the interesting token b'\\xa0\\xe9\\x99\\xa4'\n",
    "    # in which \\xe9 is the start of a 3-byte UTF-8 character and \\xa0 is a continuation byte\n",
    "    prompt = \" ƒ†Èô§\"\n",
    "    p, o = enc.decode_with_offsets(enc.encode(prompt))\n",
    "    assert p == prompt\n",
    "    assert o == [0, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0d16795",
   "metadata": {},
   "source": [
    "## **tiktoken/tests/<span style='color:orange'> **test_pickle.py** </span>**\n",
    "\n",
    "**one-liner summary:** <br><br>\n",
    "This code verifies that a tiktoken Encoding object can be safely serialized and deserialized using Python‚Äôs pickle module without losing any of its tokenization behavior. This matters because reliable serialization is essential for saving, loading, transferring, caching, or distributing tokenizer models across processes or machines.\n",
    "\n",
    "<br>This test ensures that a tokenizer‚Äîbuilt-in or custom‚Äîcan be serialized and deserialized through Python‚Äôs pickle without causing tokenization inconsistencies, which is crucial for reliably saving, sharing, and loading tokenizers in real systems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d654779",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "\n",
    "\n",
    "def test_pickle():\n",
    "    import pickle\n",
    "\n",
    "    enc_old = tiktoken.get_encoding(\"r50k_base\")\n",
    "    enc_new = pickle.loads(pickle.dumps(enc_old))\n",
    "    assert enc_old.encode(\"hello world\") == enc_new.encode(\"hello world\")\n",
    "\n",
    "    enc_old = tiktoken.Encoding(\n",
    "        name=\"custom_enc\",\n",
    "        pat_str=enc_old._pat_str,\n",
    "        mergeable_ranks=enc_old._mergeable_ranks,\n",
    "        special_tokens={\"<|pickle|>\": 100_000},\n",
    "    )\n",
    "    enc_new = pickle.loads(pickle.dumps(enc_old))\n",
    "    assert enc_old.encode(\"hello world\") == enc_new.encode(\"hello world\")\n",
    "    assert (\n",
    "        enc_old.encode(\"<|pickle|>\", allowed_special=\"all\")\n",
    "        == enc_new.encode(\"<|pickle|>\", allowed_special=\"all\")\n",
    "        == [100_000]\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d56fbad",
   "metadata": {},
   "source": [
    "## **tiktoken/tests/<span style='color:orange'> **test_simple_public.py** </span>**\n",
    "\n",
    "**one-liner summary:** <br>\n",
    "\n",
    "This test module verifies that tiktoken encodings correctly encode/decode text (including special tokens), map models to the right encodings, and avoid importing an optional dependency at import-time ‚Äî ensuring correctness, compatibility, and lightweight behavior.<br><br>\n",
    "\n",
    "**Why this matters:**<br>\n",
    "- Correct tokenization is foundational: wrong token IDs or decoding breaks models, causes incorrect token counts (affects batching, truncation, costs), and corrupts inputs/outputs.\n",
    "- Model‚Üíencoder mapping must be stable so code that chooses encoders based on a model name behaves predictably.\n",
    "- Avoiding accidental optional imports keeps installs lightweight, prevents surprising side effects, and reduces dependency/packaging problems in downstream projects.\n",
    "Together these tests protect reliability and portability for systems that rely on tiktoken."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df8968c8",
   "metadata": {},
   "source": [
    "**What it does (summary):**\n",
    "- Loads specific encodings (\"gpt2\" and \"cl100k_base\"), asserts known encode/decode results for \"hello world\" and for a string containing the special token <|endoftext|> (using allowed_special=\"all\").\n",
    "- Iterates every publicly available encoding name from tiktoken.list_encoding_names() and, for tokens 0..9999, checks a single-token round-trip: decode the single-token bytes then re-encode them, asserting the token ID is preserved.\n",
    "\n",
    "**Why it matters:**\n",
    "- The concrete assertions for \"hello world\" and the <|endoftext|> case ensure canonical, expected mappings for well-known encoders ‚Äî a regression here would indicate the tokenizer or its vocab changed unexpectedly.\n",
    "- The single-token round-trip loop verifies every encoding‚Äôs basic encode/decode invariants for many token IDs: it catches issues where decode_single_token_bytes and encode_single_token are not inverse operations (which would break token-level manipulation, detokenization, byte-level operations).\n",
    "- Overall, it enforces deterministic, reversible behavior of tokenizers ‚Äî essential for correctness when token-level operations are used (e.g., token counting, embeddings, model I/O, byte-level protocols)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d85f8834",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import sys\n",
    "\n",
    "import tiktoken\n",
    "\n",
    "\n",
    "def test_simple():\n",
    "    # Note that there are more actual tests, they're just not currently public :-)\n",
    "    enc = tiktoken.get_encoding(\"gpt2\")\n",
    "    assert enc.encode(\"hello world\") == [31373, 995]\n",
    "    assert enc.decode([31373, 995]) == \"hello world\"\n",
    "    assert enc.encode(\"hello <|endoftext|>\", allowed_special=\"all\") == [31373, 220, 50256]\n",
    "\n",
    "    enc = tiktoken.get_encoding(\"cl100k_base\")\n",
    "    assert enc.encode(\"hello world\") == [15339, 1917]\n",
    "    assert enc.decode([15339, 1917]) == \"hello world\"\n",
    "    assert enc.encode(\"hello <|endoftext|>\", allowed_special=\"all\") == [15339, 220, 100257]\n",
    "\n",
    "    for enc_name in tiktoken.list_encoding_names():\n",
    "        enc = tiktoken.get_encoding(enc_name)\n",
    "        for token in range(10_000):\n",
    "            assert enc.encode_single_token(enc.decode_single_token_bytes(token)) == token"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16c17140",
   "metadata": {},
   "source": [
    "**What it does (summary):**\n",
    "- Calls tiktoken.encoding_for_model(...) for several model identifiers and asserts the returned encoder names match the expected canonical encodings (e.g., \"gpt2\" ‚Üí \"gpt2\", \"text-davinci-003\" ‚Üí \"p50k_base\", \"gpt-3.5-turbo-0301\" ‚Üí \"cl100k_base\").\n",
    "\n",
    "**Why it matters:**\n",
    "- Many codepaths pick an encoder based on the model name. This test ensures the library maps models to the correct tokenizer. If mappings drift, tokenization will be inconsistent with how the model was trained ‚Äî leading to incorrect token IDs, mismatched inputs, different token counts, or degraded model behavior.\n",
    "- This is also crucial for billing/limits (accurate token counting) and for any logic that depends on encoder-specific behavior (special tokens, tokenization granularity)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d758cf2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_encoding_for_model():\n",
    "    enc = tiktoken.encoding_for_model(\"gpt2\")\n",
    "    assert enc.name == \"gpt2\"\n",
    "    enc = tiktoken.encoding_for_model(\"text-davinci-003\")\n",
    "    assert enc.name == \"p50k_base\"\n",
    "    enc = tiktoken.encoding_for_model(\"text-davinci-edit-001\")\n",
    "    assert enc.name == \"p50k_edit\"\n",
    "    enc = tiktoken.encoding_for_model(\"gpt-3.5-turbo-0301\")\n",
    "    assert enc.name == \"cl100k_base\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b88f4f59",
   "metadata": {},
   "source": [
    "**What it does (summary):**\n",
    "- Builds a short Python snippet that imports tiktoken and asserts \"blobfile\" is not present in sys.modules. It then runs that snippet in a fresh subprocess (i.e., a clean Python process) to ensure importing tiktoken does not automatically import the optional blobfile package.\n",
    "\n",
    "**Why it matters:**\n",
    "- Some packages include optional dependencies for extended features; if those optional modules are imported unconditionally at top-level, they force users to install extra packages or cause import-time failures. This test ensures tiktoken import is lightweight and side-effect free with respect to blobfile.\n",
    "- That prevents surprise dependency bloat, avoids packaging/CI problems, and reduces risk of import-time errors in environments where optional libs are absent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f5de7f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_optional_blobfile_dependency():\n",
    "    prog = \"\"\"\n",
    "import tiktoken\n",
    "import sys\n",
    "assert \"blobfile\" not in sys.modules\n",
    "\"\"\"\n",
    "    subprocess.check_call([sys.executable, \"-c\", prog])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4de23792",
   "metadata": {},
   "source": [
    "# **tiktoken/<span style='color:black'> **tiktoken** </span>**\n",
    "\n",
    "- _ init _.py\n",
    "- _educational.py\n",
    "- core.py\n",
    "- load.py\n",
    "- model.py\n",
    "- py.typed\n",
    "- registry.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a75265a3",
   "metadata": {},
   "source": [
    "## **tiktoken/tiktoken/<span style='color:black'> **_ init _.py** </span>**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28ed4673",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62f9b384",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the public API of tiktoken\n",
    "from .core import Encoding as Encoding\n",
    "from .model import encoding_for_model as encoding_for_model\n",
    "from .model import encoding_name_for_model as encoding_name_for_model\n",
    "from .registry import get_encoding as get_encoding\n",
    "from .registry import list_encoding_names as list_encoding_names\n",
    "\n",
    "__version__ = \"0.12.0\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e545615",
   "metadata": {},
   "source": [
    "## **tiktoken/tiktoken/<span style='color:black'> **_educational.py** </span>**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ff6feed",
   "metadata": {},
   "source": [
    "The educational implementation of the byte pair encoding algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "658f42ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"This is an educational implementation of the byte pair encoding algorithm.\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "import collections\n",
    "\n",
    "import regex\n",
    "\n",
    "import tiktoken\n",
    "\n",
    "\n",
    "class SimpleBytePairEncoding:\n",
    "    def __init__(self, *, pat_str: str, mergeable_ranks: dict[bytes, int]) -> None:\n",
    "        \"\"\"Creates an Encoding object.\"\"\"\n",
    "        # A regex pattern string that is used to split the input text\n",
    "        self.pat_str = pat_str\n",
    "        # A dictionary mapping token bytes to their ranks. The ranks correspond to merge priority\n",
    "        self.mergeable_ranks = mergeable_ranks\n",
    "\n",
    "        self._decoder = {token: token_bytes for token_bytes, token in mergeable_ranks.items()}\n",
    "        self._pat = regex.compile(pat_str)\n",
    "\n",
    "    def encode(self, text: str, visualise: str | None = \"colour\") -> list[int]:\n",
    "        \"\"\"Encodes a string into tokens.\n",
    "\n",
    "        >>> enc.encode(\"hello world\")\n",
    "        [388, 372]\n",
    "        \"\"\"\n",
    "        # Use the regex to split the text into (approximately) words\n",
    "        words = self._pat.findall(text)\n",
    "        tokens = []\n",
    "        for word in words:\n",
    "            # Turn each word into tokens, using the byte pair encoding algorithm\n",
    "            word_bytes = word.encode(\"utf-8\")\n",
    "            word_tokens = bpe_encode(self.mergeable_ranks, word_bytes, visualise=visualise)\n",
    "            tokens.extend(word_tokens)\n",
    "        return tokens\n",
    "\n",
    "    def decode_bytes(self, tokens: list[int]) -> bytes:\n",
    "        \"\"\"Decodes a list of tokens into bytes.\n",
    "\n",
    "        >>> enc.decode_bytes([388, 372])\n",
    "        b'hello world'\n",
    "        \"\"\"\n",
    "        return b\"\".join(self._decoder[token] for token in tokens)\n",
    "\n",
    "    def decode(self, tokens: list[int]) -> str:\n",
    "        \"\"\"Decodes a list of tokens into a string.\n",
    "\n",
    "        Decoded bytes are not guaranteed to be valid UTF-8. In that case, we replace\n",
    "        the invalid bytes with the replacement character \"ÔøΩ\".\n",
    "\n",
    "        >>> enc.decode([388, 372])\n",
    "        'hello world'\n",
    "        \"\"\"\n",
    "        return self.decode_bytes(tokens).decode(\"utf-8\", errors=\"replace\")\n",
    "\n",
    "    def decode_tokens_bytes(self, tokens: list[int]) -> list[bytes]:\n",
    "        \"\"\"Decodes a list of tokens into a list of bytes.\n",
    "\n",
    "        Useful for visualising how a string is tokenised.\n",
    "\n",
    "        >>> enc.decode_tokens_bytes([388, 372])\n",
    "        [b'hello', b' world']\n",
    "        \"\"\"\n",
    "        return [self._decoder[token] for token in tokens]\n",
    "\n",
    "    @staticmethod\n",
    "    def train(training_data: str, vocab_size: int, pat_str: str):\n",
    "        \"\"\"Train a BPE tokeniser on some data!\"\"\"\n",
    "        mergeable_ranks = bpe_train(data=training_data, vocab_size=vocab_size, pat_str=pat_str)\n",
    "        return SimpleBytePairEncoding(pat_str=pat_str, mergeable_ranks=mergeable_ranks)\n",
    "\n",
    "    @staticmethod\n",
    "    def from_tiktoken(encoding):\n",
    "        if isinstance(encoding, str):\n",
    "            encoding = tiktoken.get_encoding(encoding)\n",
    "        return SimpleBytePairEncoding(\n",
    "            pat_str=encoding._pat_str, mergeable_ranks=encoding._mergeable_ranks\n",
    "        )\n",
    "\n",
    "\n",
    "def bpe_encode(\n",
    "    mergeable_ranks: dict[bytes, int], input: bytes, visualise: str | None = \"colour\"\n",
    ") -> list[int]:\n",
    "    parts = [bytes([b]) for b in input]\n",
    "    while True:\n",
    "        # See the intermediate merges play out!\n",
    "        if visualise:\n",
    "            if visualise in [\"colour\", \"color\"]:\n",
    "                visualise_tokens(parts)\n",
    "            elif visualise == \"simple\":\n",
    "                print(parts)\n",
    "\n",
    "        # Iterate over all pairs and find the pair we want to merge the most\n",
    "        min_idx = None\n",
    "        min_rank = None\n",
    "        for i, pair in enumerate(zip(parts[:-1], parts[1:])):\n",
    "            rank = mergeable_ranks.get(pair[0] + pair[1])\n",
    "            if rank is not None and (min_rank is None or rank < min_rank):\n",
    "                min_idx = i\n",
    "                min_rank = rank\n",
    "\n",
    "        # If there were no pairs we could merge, we're done!\n",
    "        if min_rank is None:\n",
    "            break\n",
    "        assert min_idx is not None\n",
    "\n",
    "        # Otherwise, merge that pair and leave the rest unchanged. Then repeat.\n",
    "        parts = parts[:min_idx] + [parts[min_idx] + parts[min_idx + 1]] + parts[min_idx + 2 :]\n",
    "\n",
    "    if visualise:\n",
    "        print()\n",
    "\n",
    "    tokens = [mergeable_ranks[part] for part in parts]\n",
    "    return tokens\n",
    "\n",
    "\n",
    "def bpe_train(\n",
    "    data: str, vocab_size: int, pat_str: str, visualise: str | None = \"colour\"\n",
    ") -> dict[bytes, int]:\n",
    "    # First, add tokens for each individual byte value\n",
    "    if vocab_size < 2**8:\n",
    "        raise ValueError(\"vocab_size must be at least 256, so we can encode all bytes\")\n",
    "    ranks = {}\n",
    "    for i in range(2**8):\n",
    "        ranks[bytes([i])] = i\n",
    "\n",
    "    # Splinter up our data into lists of bytes\n",
    "    # data = \"Hello world\"\n",
    "    # words = [\n",
    "    #     [b'H', b'e', b'l', b'l', b'o'],\n",
    "    #     [b' ', b'w', b'o', b'r', b'l', b'd']\n",
    "    # ]\n",
    "    words: list[list[bytes]] = [\n",
    "        [bytes([b]) for b in word.encode(\"utf-8\")] for word in regex.findall(pat_str, data)\n",
    "    ]\n",
    "\n",
    "    # Now, use our data to figure out which merges we should make\n",
    "    while len(ranks) < vocab_size:\n",
    "        # Find the most common pair. This will become our next token\n",
    "        stats = collections.Counter()\n",
    "        for piece in words:\n",
    "            for pair in zip(piece[:-1], piece[1:]):\n",
    "                stats[pair] += 1\n",
    "\n",
    "        most_common_pair = max(stats, key=lambda x: stats[x])\n",
    "        token_bytes = most_common_pair[0] + most_common_pair[1]\n",
    "        token = len(ranks)\n",
    "        # Add the new token!\n",
    "        ranks[token_bytes] = token\n",
    "\n",
    "        # Now merge that most common pair in all the words. That is, update our training data\n",
    "        # to reflect our decision to make that pair into a new token.\n",
    "        new_words = []\n",
    "        for word in words:\n",
    "            new_word = []\n",
    "            i = 0\n",
    "            while i < len(word) - 1:\n",
    "                if (word[i], word[i + 1]) == most_common_pair:\n",
    "                    # We found our pair! Merge it\n",
    "                    new_word.append(token_bytes)\n",
    "                    i += 2\n",
    "                else:\n",
    "                    new_word.append(word[i])\n",
    "                    i += 1\n",
    "            if i == len(word) - 1:\n",
    "                new_word.append(word[i])\n",
    "            new_words.append(new_word)\n",
    "        words = new_words\n",
    "\n",
    "        # See the intermediate merges play out!\n",
    "        if visualise:\n",
    "            print(f\"The current most common pair is {most_common_pair[0]} + {most_common_pair[1]}\")\n",
    "            print(f\"So we made {token_bytes} our {len(ranks)}th token\")\n",
    "            if visualise in [\"colour\", \"color\"]:\n",
    "                print(\"Now the first fifty words in our training data look like:\")\n",
    "                visualise_tokens([token for word in words[:50] for token in word])\n",
    "            elif visualise == \"simple\":\n",
    "                print(\"Now the first twenty words in our training data look like:\")\n",
    "                for word in words[:20]:\n",
    "                    print(word)\n",
    "            print(\"\\n\")\n",
    "\n",
    "    return ranks\n",
    "\n",
    "\n",
    "def visualise_tokens(token_values: list[bytes]) -> None:\n",
    "    background = [f\"\\u001b[48;5;{i}m\" for i in [167, 179, 185, 77, 80, 68, 134]]\n",
    "    # If token boundaries do not occur at unicode character boundaries, it's unclear how best to\n",
    "    # visualise the token. Here, we'll just use the unicode replacement character to represent some\n",
    "    # fraction of a character.\n",
    "    unicode_token_values = [x.decode(\"utf-8\", errors=\"replace\") for x in token_values]\n",
    "\n",
    "    running_length = 0\n",
    "    last_color = None\n",
    "    for token in unicode_token_values:\n",
    "        color = background[running_length % len(background)]\n",
    "        if color == last_color:\n",
    "            color = background[(running_length + 1) % len(background)]\n",
    "            assert color != last_color\n",
    "        last_color = color\n",
    "        running_length += len(token)\n",
    "        print(color + token, end=\"\")\n",
    "    print(\"\\u001b[0m\")\n",
    "\n",
    "\n",
    "def train_simple_encoding():\n",
    "    gpt2_pattern = (\n",
    "        r\"\"\"'s|'t|'re|'ve|'m|'ll|'d| ?[\\p{L}]+| ?[\\p{N}]+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+\"\"\"\n",
    "    )\n",
    "    with open(__file__) as f:\n",
    "        data = f.read()\n",
    "\n",
    "    enc = SimpleBytePairEncoding.train(data, vocab_size=600, pat_str=gpt2_pattern)\n",
    "\n",
    "    print(\"This is the sequence of merges performed in order to encode 'hello world':\")\n",
    "    tokens = enc.encode(\"hello world\")\n",
    "    assert enc.decode(tokens) == \"hello world\"\n",
    "    assert enc.decode_bytes(tokens) == b\"hello world\"\n",
    "    assert enc.decode_tokens_bytes(tokens) == [b\"hello\", b\" world\"]\n",
    "\n",
    "    return enc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1527e26f",
   "metadata": {},
   "source": [
    "## **tiktoken/tiktoken/<span style='color:black'> **core.py** </span>**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09472362",
   "metadata": {},
   "source": [
    "**ONE-LINER SUMMARY:**<br>\n",
    "\n",
    "This code implements a complete Byte-Pair-Encoding (BPE) tokenizer that converts text ‚Üî tokens safely, efficiently, and with strict control over special tokens to prevent misuse or injection attacks.<br><br>\n",
    "\n",
    "This code provides a fast, safe, feature-complete BPE tokenizer that handles normal text, special-token security, parallel encoding/decoding, and compatibility with LLM training and inference.<br><br>\n",
    "\n",
    "\n",
    "**HIGH-LEVEL RESPONSIBILITY OF THE CODE:**<br>\n",
    "\n",
    "This file wraps a fast Rust BPE engine (CoreBPE) and provides Python-friendly methods to:\n",
    "\n",
    "1. Encode text into tokens (normal strings ‚Üí token IDs)\n",
    "2.\tDecode tokens back into text\n",
    "3.\tHandle special tokens safely (e.g., <|endoftext|>, FIM tokens)\n",
    "4.\tRun tokenization in parallel\n",
    "5.\tValidate, cache, and registry-load encodings\n",
    "6.\tProvide stable interfaces for ML model training + inference\n",
    "\n",
    "It is the public-facing implementation behind OpenAI‚Äôs tiktoken library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89786974",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import functools\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from typing import TYPE_CHECKING, AbstractSet, Collection, Literal, NoReturn, Sequence\n",
    "\n",
    "from tiktoken import _tiktoken\n",
    "\n",
    "if TYPE_CHECKING:\n",
    "    import re\n",
    "\n",
    "    import numpy as np\n",
    "    import numpy.typing as npt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccc71ac6",
   "metadata": {},
   "source": [
    "*class Encodings*<br><br>\n",
    "\n",
    "**Overall Purpose**<br>\n",
    "\n",
    "- Represents a specific tokenizer configuration *(regex rules, merges, and special tokens)*, and exposes all encode/decode operations.\n",
    "\n",
    "**Why It Matters**<br>\n",
    "\n",
    "- This is the central object used everywhere in LLM applications (OpenAI, HuggingFace, etc.). Without it, models couldn‚Äôt reliably convert between text and tokens."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b77c0ac2",
   "metadata": {},
   "source": [
    "*\"class Encodings\"*<br><br><span style='color:GREEN'> **BEGINS**</span>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc3f4ca6",
   "metadata": {},
   "source": [
    "**What it does**\n",
    "- Stores regex, BPE ranks, and special token mappings.\n",
    "- Ensures vocabulary size is consistent.\n",
    "- Builds the underlying fast Rust CoreBPE engine.\n",
    "\n",
    "**Why it matters**\n",
    "- Misconfigured tokenizers break models. This step guarantees vocabulary integrity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "064af168",
   "metadata": {},
   "outputs": [],
   "source": [
    "def __init__(\n",
    "        self,\n",
    "        name: str,\n",
    "        *,\n",
    "        pat_str: str,\n",
    "        mergeable_ranks: dict[bytes, int],\n",
    "        special_tokens: dict[str, int],\n",
    "        explicit_n_vocab: int | None = None,\n",
    "    ):\n",
    "        \"\"\"Creates an Encoding object.\n",
    "\n",
    "        See openai_public.py for examples of how to construct an Encoding object.\n",
    "\n",
    "        Args:\n",
    "            name: The name of the encoding. It should be clear from the name of the encoding\n",
    "                what behaviour to expect, in particular, encodings with different special tokens\n",
    "                should have different names.\n",
    "            pat_str: A regex pattern string that is used to split the input text.\n",
    "            mergeable_ranks: A dictionary mapping mergeable token bytes to their ranks. The ranks\n",
    "                must correspond to merge priority.\n",
    "            special_tokens: A dictionary mapping special token strings to their token values.\n",
    "            explicit_n_vocab: The number of tokens in the vocabulary. If provided, it is checked\n",
    "                that the number of mergeable tokens and special tokens is equal to this number.\n",
    "        \"\"\"\n",
    "        self.name = name\n",
    "\n",
    "        self._pat_str = pat_str\n",
    "        self._mergeable_ranks = mergeable_ranks\n",
    "        self._special_tokens = special_tokens\n",
    "\n",
    "        self.max_token_value = max(\n",
    "            max(mergeable_ranks.values()), max(special_tokens.values(), default=0)\n",
    "        )\n",
    "        if explicit_n_vocab:\n",
    "            assert len(mergeable_ranks) + len(special_tokens) == explicit_n_vocab\n",
    "            assert self.max_token_value == explicit_n_vocab - 1\n",
    "\n",
    "        self._core_bpe = _tiktoken.CoreBPE(mergeable_ranks, special_tokens, pat_str)\n",
    "\n",
    "def __repr__(self) -> str:\n",
    "        return f\"<Encoding {self.name!r}>\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5fba68e",
   "metadata": {},
   "source": [
    "    # ====================\n",
    "    # Encoding\n",
    "    # ===================="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ea7b031",
   "metadata": {},
   "source": [
    "**What**\n",
    "- Encodes text into tokens ignoring special tokens entirely.\n",
    "\n",
    "**Why**\n",
    "- Faster and simpler path when you know text has no special tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18cff1d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_ordinary(self, text: str) -> list[int]:\n",
    "        \"\"\"Encodes a string into tokens, ignoring special tokens.\n",
    "\n",
    "        This is equivalent to `encode(text, disallowed_special=())` (but slightly faster).\n",
    "\n",
    "        ```\n",
    "        >>> enc.encode_ordinary(\"hello world\")\n",
    "        [31373, 995]\n",
    "        \"\"\"\n",
    "        try:\n",
    "            return self._core_bpe.encode_ordinary(text)\n",
    "        except UnicodeEncodeError:\n",
    "            # See comment in encode\n",
    "            text = text.encode(\"utf-16\", \"surrogatepass\").decode(\"utf-16\", \"replace\")\n",
    "            return self._core_bpe.encode_ordinary(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1112e458",
   "metadata": {},
   "source": [
    "**What**\n",
    "\n",
    "Main secure encoder:\n",
    "- Detects special tokens in text\n",
    "- Allows or blocks them\n",
    "- Throws errors if disallowed tokens are present\n",
    "- Uses Rust BPE for speed\n",
    "\n",
    "**Why**\n",
    "- Prevents prompt injection and accidental triggering of model modes (like FIM or EOT tokens). This is critical for safety."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7713960b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(\n",
    "        self,\n",
    "        text: str,\n",
    "        *,\n",
    "        allowed_special: Literal[\"all\"] | AbstractSet[str] = set(),  # noqa: B006\n",
    "        disallowed_special: Literal[\"all\"] | Collection[str] = \"all\",\n",
    "    ) -> list[int]:\n",
    "        \"\"\"Encodes a string into tokens.\n",
    "\n",
    "        Special tokens are artificial tokens used to unlock capabilities from a model,\n",
    "        such as fill-in-the-middle. So we want to be careful about accidentally encoding special\n",
    "        tokens, since they can be used to trick a model into doing something we don't want it to do.\n",
    "\n",
    "        Hence, by default, encode will raise an error if it encounters text that corresponds\n",
    "        to a special token. This can be controlled on a per-token level using the `allowed_special`\n",
    "        and `disallowed_special` parameters. In particular:\n",
    "        - Setting `disallowed_special` to () will prevent this function from raising errors and\n",
    "          cause all text corresponding to special tokens to be encoded as natural text.\n",
    "        - Setting `allowed_special` to \"all\" will cause this function to treat all text\n",
    "          corresponding to special tokens to be encoded as special tokens.\n",
    "\n",
    "        ```\n",
    "        >>> enc.encode(\"hello world\")\n",
    "        [31373, 995]\n",
    "        >>> enc.encode(\"<|endoftext|>\", allowed_special={\"<|endoftext|>\"})\n",
    "        [50256]\n",
    "        >>> enc.encode(\"<|endoftext|>\", allowed_special=\"all\")\n",
    "        [50256]\n",
    "        >>> enc.encode(\"<|endoftext|>\")\n",
    "        # Raises ValueError\n",
    "        >>> enc.encode(\"<|endoftext|>\", disallowed_special=())\n",
    "        [27, 91, 437, 1659, 5239, 91, 29]\n",
    "        ```\n",
    "        \"\"\"\n",
    "        if allowed_special == \"all\":\n",
    "            allowed_special = self.special_tokens_set\n",
    "        if disallowed_special == \"all\":\n",
    "            disallowed_special = self.special_tokens_set - allowed_special\n",
    "        if disallowed_special:\n",
    "            if not isinstance(disallowed_special, frozenset):\n",
    "                disallowed_special = frozenset(disallowed_special)\n",
    "            if match := _special_token_regex(disallowed_special).search(text):\n",
    "                raise_disallowed_special_token(match.group())\n",
    "\n",
    "        try:\n",
    "            return self._core_bpe.encode(text, allowed_special)\n",
    "        except UnicodeEncodeError:\n",
    "            # BPE operates on bytes, but the regex operates on unicode. If we pass a str that is\n",
    "            # invalid UTF-8 to Rust, it will rightfully complain. Here we do a quick and dirty\n",
    "            # fixup for any surrogate pairs that may have sneaked their way into the text.\n",
    "            # Technically, this introduces a place where encode + decode doesn't roundtrip a Python\n",
    "            # string, but given that this is input we want to support, maybe that's okay.\n",
    "            # Also we use errors=\"replace\" to handle weird things like lone surrogates.\n",
    "            text = text.encode(\"utf-16\", \"surrogatepass\").decode(\"utf-16\", \"replace\")\n",
    "            return self._core_bpe.encode(text, allowed_special)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df78e46f",
   "metadata": {},
   "source": [
    "**What**\n",
    "\n",
    "- Same as encode but returns a NumPy array instead of list.\n",
    "\n",
    "**Why**\n",
    "\n",
    "- Avoids memory copies ‚Üí massive speed benefits for high-throughput systems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83f659dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_to_numpy(\n",
    "        self,\n",
    "        text: str,\n",
    "        *,\n",
    "        allowed_special: Literal[\"all\"] | AbstractSet[str] = set(),  # noqa: B006\n",
    "        disallowed_special: Literal[\"all\"] | Collection[str] = \"all\",\n",
    "    ) -> npt.NDArray[np.uint32]:\n",
    "        \"\"\"Encodes a string into tokens, returning a numpy array.\n",
    "\n",
    "        Avoids the overhead of copying the token buffer into a Python list.\n",
    "        \"\"\"\n",
    "        if allowed_special == \"all\":\n",
    "            allowed_special = self.special_tokens_set\n",
    "        if disallowed_special == \"all\":\n",
    "            disallowed_special = self.special_tokens_set - allowed_special\n",
    "        if disallowed_special:\n",
    "            if not isinstance(disallowed_special, frozenset):\n",
    "                disallowed_special = frozenset(disallowed_special)\n",
    "            if match := _special_token_regex(disallowed_special).search(text):\n",
    "                raise_disallowed_special_token(match.group())\n",
    "\n",
    "        import numpy as np\n",
    "\n",
    "        buffer = self._core_bpe.encode_to_tiktoken_buffer(text, allowed_special)\n",
    "        return np.frombuffer(buffer, dtype=np.uint32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d87c828f",
   "metadata": {},
   "source": [
    "**What**\n",
    "\n",
    "- Run tokenization in parallel threads.\n",
    "\n",
    "**Why**\n",
    "\n",
    "- Huge speedup when encoding thousands of prompts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c4caead",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_ordinary_batch(self, text: list[str], *, num_threads: int = 8) -> list[list[int]]:\n",
    "        \"\"\"Encodes a list of strings into tokens, in parallel, ignoring special tokens.\n",
    "\n",
    "        This is equivalent to `encode_batch(text, disallowed_special=())` (but slightly faster).\n",
    "\n",
    "        ```\n",
    "        >>> enc.encode_ordinary_batch([\"hello world\", \"goodbye world\"])\n",
    "        [[31373, 995], [11274, 16390, 995]]\n",
    "        ```\n",
    "        \"\"\"\n",
    "        encoder = functools.partial(self.encode_ordinary)\n",
    "        with ThreadPoolExecutor(num_threads) as e:\n",
    "            return list(e.map(encoder, text))\n",
    "\n",
    "def encode_batch(\n",
    "        self,\n",
    "        text: list[str],\n",
    "        *,\n",
    "        num_threads: int = 8,\n",
    "        allowed_special: Literal[\"all\"] | AbstractSet[str] = set(),  # noqa: B006\n",
    "        disallowed_special: Literal[\"all\"] | Collection[str] = \"all\",\n",
    "    ) -> list[list[int]]:\n",
    "        \"\"\"Encodes a list of strings into tokens, in parallel.\n",
    "\n",
    "        See `encode` for more details on `allowed_special` and `disallowed_special`.\n",
    "\n",
    "        ```\n",
    "        >>> enc.encode_batch([\"hello world\", \"goodbye world\"])\n",
    "        [[31373, 995], [11274, 16390, 995]]\n",
    "        ```\n",
    "        \"\"\"\n",
    "        if allowed_special == \"all\":\n",
    "            allowed_special = self.special_tokens_set\n",
    "        if disallowed_special == \"all\":\n",
    "            disallowed_special = self.special_tokens_set - allowed_special\n",
    "        if not isinstance(disallowed_special, frozenset):\n",
    "            disallowed_special = frozenset(disallowed_special)\n",
    "\n",
    "        encoder = functools.partial(\n",
    "            self.encode, allowed_special=allowed_special, disallowed_special=disallowed_special\n",
    "        )\n",
    "        with ThreadPoolExecutor(num_threads) as e:\n",
    "            return list(e.map(encoder, text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a35b187d",
   "metadata": {},
   "source": [
    "**What**\n",
    "\n",
    "Returns:\n",
    "- Stable tokens (guaranteed prefix)\n",
    "- All possible completions\n",
    "\n",
    "**Why**\n",
    "\n",
    "- Used internally for beam search, completion prediction, and advanced sampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc0e11a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_with_unstable(\n",
    "        self,\n",
    "        text: str,\n",
    "        *,\n",
    "        allowed_special: Literal[\"all\"] | AbstractSet[str] = set(),  # noqa: B006\n",
    "        disallowed_special: Literal[\"all\"] | Collection[str] = \"all\",\n",
    "    ) -> tuple[list[int], list[list[int]]]:\n",
    "        \"\"\"Encodes a string into stable tokens and possible completion sequences.\n",
    "\n",
    "        Note that the stable tokens will only represent a substring of `text`.\n",
    "\n",
    "        See `encode` for more details on `allowed_special` and `disallowed_special`.\n",
    "\n",
    "        This API should itself be considered unstable.\n",
    "\n",
    "        ```\n",
    "        >>> enc.encode_with_unstable(\"hello fanta\")\n",
    "        ([31373], [(277, 4910), (5113, 265), ..., (8842,)])\n",
    "\n",
    "        >>> text = \"...\"\n",
    "        >>> stable_tokens, completions = enc.encode_with_unstable(text)\n",
    "        >>> assert text.encode().startswith(enc.decode_bytes(stable_tokens))\n",
    "        >>> assert all(enc.decode_bytes(stable_tokens + seq).startswith(text.encode()) for seq in completions)\n",
    "        ```\n",
    "        \"\"\"\n",
    "        if allowed_special == \"all\":\n",
    "            allowed_special = self.special_tokens_set\n",
    "        if disallowed_special == \"all\":\n",
    "            disallowed_special = self.special_tokens_set - allowed_special\n",
    "        if disallowed_special:\n",
    "            if not isinstance(disallowed_special, frozenset):\n",
    "                disallowed_special = frozenset(disallowed_special)\n",
    "            if match := _special_token_regex(disallowed_special).search(text):\n",
    "                raise_disallowed_special_token(match.group())\n",
    "\n",
    "        return self._core_bpe.encode_with_unstable(text, allowed_special)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ce66090",
   "metadata": {},
   "source": [
    "**What**\n",
    "\n",
    "- Convert exactly one token (text or bytes) ‚Üí its ID.\n",
    "\n",
    "**Why**\n",
    "\n",
    "- Used for debugging, tokenizer introspection, or low-level BPE analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feadc626",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_single_token(self, text_or_bytes: str | bytes) -> int:\n",
    "        \"\"\"Encodes text corresponding to a single token to its token value.\n",
    "\n",
    "        NOTE: this will encode all special tokens.\n",
    "\n",
    "        Raises `KeyError` if the token is not in the vocabulary.\n",
    "\n",
    "        ```\n",
    "        >>> enc.encode_single_token(\"hello\")\n",
    "        31373\n",
    "        ```\n",
    "        \"\"\"\n",
    "        if isinstance(text_or_bytes, str):\n",
    "            text_or_bytes = text_or_bytes.encode(\"utf-8\")\n",
    "        return self._core_bpe.encode_single_token(text_or_bytes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6bf9aff",
   "metadata": {},
   "source": [
    "    # ====================\n",
    "    # Decoding\n",
    "    # ===================="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19bdc847",
   "metadata": {},
   "source": [
    "**What** ‚Üí bytes\n",
    "\n",
    "- Fastest decoding path.\n",
    "\n",
    "**Why**\n",
    "\n",
    "- Needed for LLM streaming output or non-UTF8 content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab61dde4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_bytes(self, tokens: Sequence[int]) -> bytes:\n",
    "        \"\"\"Decodes a list of tokens into bytes.\n",
    "\n",
    "        ```\n",
    "        >>> enc.decode_bytes([31373, 995])\n",
    "        b'hello world'\n",
    "        ```\n",
    "        \"\"\"\n",
    "        return self._core_bpe.decode_bytes(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3bafb8d",
   "metadata": {},
   "source": [
    "**What**\n",
    "\n",
    "- Full decode to UTF-8 string.\n",
    "\n",
    "**Why**\n",
    "\n",
    "- This is how tokens become human-readable model output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18215ac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode(self, tokens: Sequence[int], errors: str = \"replace\") -> str:\n",
    "        \"\"\"Decodes a list of tokens into a string.\n",
    "\n",
    "        WARNING: the default behavior of this function is lossy, since decoded bytes are not\n",
    "        guaranteed to be valid UTF-8. You can control this behavior using the `errors` parameter,\n",
    "        for instance, setting `errors=strict`.\n",
    "\n",
    "        ```\n",
    "        >>> enc.decode([31373, 995])\n",
    "        'hello world'\n",
    "        ```\n",
    "        \"\"\"\n",
    "        return self._core_bpe.decode_bytes(tokens).decode(\"utf-8\", errors=errors)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92c4626a",
   "metadata": {},
   "source": [
    "**What**\n",
    "\n",
    "- Look up bytes for a single token.\n",
    "\n",
    "**Why**\n",
    "\n",
    "- Used to inspect exactly what a token represents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "913ab193",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_single_token_bytes(self, token: int) -> bytes:\n",
    "        \"\"\"Decodes a token into bytes.\n",
    "\n",
    "        NOTE: this will decode all special tokens.\n",
    "\n",
    "        Raises `KeyError` if the token is not in the vocabulary.\n",
    "\n",
    "        ```\n",
    "        >>> enc.decode_single_token_bytes(31373)\n",
    "        b'hello'\n",
    "        ```\n",
    "        \"\"\"\n",
    "        return self._core_bpe.decode_single_token_bytes(token)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25696137",
   "metadata": {},
   "source": [
    "**What**\n",
    "\n",
    "- List each token‚Äôs raw bytes.\n",
    "\n",
    "**Why**\n",
    "\n",
    "- Great for visualizing token boundaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d065e88f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_tokens_bytes(self, tokens: Sequence[int]) -> list[bytes]:\n",
    "        \"\"\"Decodes a list of tokens into a list of bytes.\n",
    "\n",
    "        Useful for visualising tokenisation.\n",
    "        >>> enc.decode_tokens_bytes([31373, 995])\n",
    "        [b'hello', b' world']\n",
    "        \"\"\"\n",
    "        return [self.decode_single_token_bytes(token) for token in tokens]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "403dbdb4",
   "metadata": {},
   "source": [
    "**What**\n",
    "\n",
    "- Returns text + index offsets where each token begins.\n",
    "\n",
    "**Why**\n",
    "\n",
    "- Vital for aligning model predictions to text (for highlighting, instruction alignment, token-level editing, etc.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbbc9aad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_with_offsets(self, tokens: Sequence[int]) -> tuple[str, list[int]]:\n",
    "        \"\"\"Decodes a list of tokens into a string and a list of offsets.\n",
    "\n",
    "        Each offset is the index into text corresponding to the start of each token.\n",
    "        If UTF-8 character boundaries do not line up with token boundaries, the offset is the index\n",
    "        of the first character that contains bytes from the token.\n",
    "\n",
    "        This will currently raise if given tokens that decode to invalid UTF-8; this behaviour may\n",
    "        change in the future to be more permissive.\n",
    "\n",
    "        >>> enc.decode_with_offsets([31373, 995])\n",
    "        ('hello world', [0, 5])\n",
    "        \"\"\"\n",
    "        token_bytes = self.decode_tokens_bytes(tokens)\n",
    "\n",
    "        text_len = 0\n",
    "        offsets = []\n",
    "        for token in token_bytes:\n",
    "            offsets.append(max(0, text_len - (0x80 <= token[0] < 0xC0)))\n",
    "            text_len += sum(1 for c in token if not 0x80 <= c < 0xC0)\n",
    "\n",
    "        # TODO: assess correctness for errors=\"ignore\" and errors=\"replace\"\n",
    "        text = b\"\".join(token_bytes).decode(\"utf-8\", errors=\"strict\")\n",
    "        return text, offsets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0620356d",
   "metadata": {},
   "source": [
    "**What**\n",
    "\n",
    "- Parallel decoding.\n",
    "\n",
    "**Why**\n",
    "\n",
    "- Speed for multi-request workloads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f39426f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_batch(\n",
    "        self, batch: Sequence[Sequence[int]], *, errors: str = \"replace\", num_threads: int = 8\n",
    "    ) -> list[str]:\n",
    "        \"\"\"Decodes a batch (list of lists of tokens) into a list of strings.\"\"\"\n",
    "        decoder = functools.partial(self.decode, errors=errors)\n",
    "        with ThreadPoolExecutor(num_threads) as e:\n",
    "            return list(e.map(decoder, batch))\n",
    "\n",
    "def decode_bytes_batch(\n",
    "        self, batch: Sequence[Sequence[int]], *, num_threads: int = 8\n",
    "    ) -> list[bytes]:\n",
    "        \"\"\"Decodes a batch (list of lists of tokens) into a list of bytes.\"\"\"\n",
    "        with ThreadPoolExecutor(num_threads) as e:\n",
    "            return list(e.map(self.decode_bytes, batch))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d770d552",
   "metadata": {},
   "source": [
    "    # ====================\n",
    "    # Miscellaneous\n",
    "    # ===================="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eee88ed9",
   "metadata": {},
   "source": [
    "Returns raw byte values for each token in vocab. Useful for debugging & introspection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84d53104",
   "metadata": {},
   "outputs": [],
   "source": [
    "def token_byte_values(self) -> list[bytes]:\n",
    "        \"\"\"Returns the list of all token byte values.\"\"\"\n",
    "        return self._core_bpe.token_byte_values()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5209287d",
   "metadata": {},
   "source": [
    "Returns <|endoftext|> token ID. Often used in training loops."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08efcaf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "@property\n",
    "def eot_token(self) -> int:\n",
    "        return self._special_tokens[\"<|endoftext|>\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c148ec96",
   "metadata": {},
   "source": [
    "Cached set of special token strings. Used by encoders for fast checking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c73de756",
   "metadata": {},
   "outputs": [],
   "source": [
    "@functools.cached_property\n",
    "def special_tokens_set(self) -> set[str]:\n",
    "        return set(self._special_tokens.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8a35a19",
   "metadata": {},
   "source": [
    "Tests whether a token ID is special."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceb8e161",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_special_token(self, token: int) -> bool:\n",
    "        assert isinstance(token, int)\n",
    "        return token in self._special_token_values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d007f07",
   "metadata": {},
   "source": [
    "Returns vocabulary size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6748e5fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "@property\n",
    "def n_vocab(self) -> int:\n",
    "        \"\"\"For backwards compatibility. Prefer to use `enc.max_token_value + 1`.\"\"\"\n",
    "        return self.max_token_value + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1108d3f1",
   "metadata": {},
   "source": [
    "    # ====================\n",
    "    # Private\n",
    "    # ===================="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "236b94f7",
   "metadata": {},
   "source": [
    "Encode text without regex splitting. Handles inner BPE operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eae5f90",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _encode_single_piece(self, text_or_bytes: str | bytes) -> list[int]:\n",
    "        \"\"\"Encodes text corresponding to bytes without a regex split.\n",
    "\n",
    "        NOTE: this will not encode any special tokens.\n",
    "\n",
    "        ```\n",
    "        >>> enc.encode_single_piece(\"helloqqqq\")\n",
    "        [31373, 38227, 38227]\n",
    "        ```\n",
    "        \"\"\"\n",
    "        if isinstance(text_or_bytes, str):\n",
    "            text_or_bytes = text_or_bytes.encode(\"utf-8\")\n",
    "        return self._core_bpe.encode_single_piece(text_or_bytes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f2aa0d4",
   "metadata": {},
   "source": [
    "Emulates the Rust BPE algorithm in Python. Ultra-low-level fast path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e78aa2fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _encode_only_native_bpe(self, text: str) -> list[int]:\n",
    "        \"\"\"Encodes a string into tokens, but do regex splitting in Python.\"\"\"\n",
    "        # We need specifically `regex` in order to compile pat_str due to e.g. \\p\n",
    "        import regex\n",
    "\n",
    "        _unused_pat = regex.compile(self._pat_str)\n",
    "        ret = []\n",
    "        for piece in regex.findall(_unused_pat, text):\n",
    "            ret.extend(self._core_bpe.encode_single_piece(piece.encode(\"utf-8\")))\n",
    "        return ret"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32ae5789",
   "metadata": {},
   "source": [
    "Pass bytes through Rust encoder. Ultra-low-level fast path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aaa2c46",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _encode_bytes(self, text: bytes) -> list[int]:\n",
    "        return self._core_bpe._encode_bytes(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b708560",
   "metadata": {},
   "source": [
    "Pickling support. Allows tokenizers to be cached, saved, and restored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07f57022",
   "metadata": {},
   "outputs": [],
   "source": [
    "def __getstate__(self) -> object:\n",
    "        import tiktoken.registry\n",
    "\n",
    "        # As an optimisation, pickle registered encodings by reference\n",
    "        if self is tiktoken.registry.ENCODINGS.get(self.name):\n",
    "            return self.name\n",
    "        return {\n",
    "            \"name\": self.name,\n",
    "            \"pat_str\": self._pat_str,\n",
    "            \"mergeable_ranks\": self._mergeable_ranks,\n",
    "            \"special_tokens\": self._special_tokens,\n",
    "        }\n",
    "\n",
    "def __setstate__(self, value: object) -> None:\n",
    "        import tiktoken.registry\n",
    "\n",
    "        if isinstance(value, str):\n",
    "            self.__dict__ = tiktoken.registry.get_encoding(value).__dict__\n",
    "            return\n",
    "        self.__init__(**value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d625d6b",
   "metadata": {},
   "source": [
    "*\"class Encodings\"*<br><br><span style='color:RED'> **ENDS**</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e6a547f",
   "metadata": {},
   "source": [
    "Creates a regex pattern matching all special tokens. Fast detection of injection attempts or forbidden tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02d4538e",
   "metadata": {},
   "outputs": [],
   "source": [
    "@functools.lru_cache(maxsize=128)\n",
    "def _special_token_regex(tokens: frozenset[str]) -> re.Pattern[str]:\n",
    "    try:\n",
    "        import regex as re\n",
    "    except ImportError:\n",
    "        import re\n",
    "    inner = \"|\".join(re.escape(token) for token in tokens)\n",
    "    return re.compile(f\"({inner})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48ba6d9e",
   "metadata": {},
   "source": [
    "Raises detailed error explaining how to handle special tokens. Safety, clarity, misuse prevention."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67e40222",
   "metadata": {},
   "outputs": [],
   "source": [
    "def raise_disallowed_special_token(token: str) -> NoReturn:\n",
    "    raise ValueError(\n",
    "        f\"Encountered text corresponding to disallowed special token {token!r}.\\n\"\n",
    "        \"If you want this text to be encoded as a special token, \"\n",
    "        f\"pass it to `allowed_special`, e.g. `allowed_special={{{token!r}, ...}}`.\\n\"\n",
    "        f\"If you want this text to be encoded as normal text, disable the check for this token \"\n",
    "        f\"by passing `disallowed_special=(enc.special_tokens_set - {{{token!r}}})`.\\n\"\n",
    "        \"To disable this check for all special tokens, pass `disallowed_special=()`.\\n\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba2c73e3",
   "metadata": {},
   "source": [
    "## **tiktoken/tiktoken/<span style='color:black'> **load.py** </span>**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "895ffdba",
   "metadata": {},
   "source": [
    "This module provides robust file I/O helpers (local/HTTP/cloud) with optional caching and integrity checks, plus converters to read/produce mergeable BPE rank tables used by tiktoken-style tokenizers (and to serialize/deserialize those tables).\n",
    "\n",
    "**One-liner**:<br>\n",
    "\n",
    "- Utilities to safely fetch/cache files and convert between DataGym-style BPE/encoder files and tiktoken-compatible mergeable-BPE rank files (and read/write those)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60991092",
   "metadata": {},
   "source": [
    "- Unifies reading from local disk, public HTTP URLs, and cloud-backed storage (GCS/S3 through blobfile) behind one function.\n",
    "- Avoids unexpected MFA/auth prompts by preferring plain HTTP for public URLs.\n",
    "- Reduces duplicated code and centralizes error messaging for missing dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ab8a8a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file(blobpath: str) -> bytes:\n",
    "    if \"://\" not in blobpath:\n",
    "        with open(blobpath, \"rb\", buffering=0) as f:\n",
    "            return f.read()\n",
    "\n",
    "    if blobpath.startswith((\"http://\", \"https://\")):\n",
    "        # avoiding blobfile for public files helps avoid auth issues, like MFA prompts.\n",
    "        import requests\n",
    "\n",
    "        resp = requests.get(blobpath)\n",
    "        resp.raise_for_status()\n",
    "        return resp.content\n",
    "\n",
    "    try:\n",
    "        import blobfile\n",
    "    except ImportError as e:\n",
    "        raise ImportError(\n",
    "            \"blobfile is not installed. Please install it by running `pip install blobfile`.\"\n",
    "        ) from e\n",
    "    with blobfile.BlobFile(blobpath, \"rb\") as f:\n",
    "        return f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd1174dd",
   "metadata": {},
   "source": [
    "**What it does**\n",
    "- Computes the SHA-256 hex digest of data and returns whether it equals expected_hash.\n",
    "\n",
    "**How it works**\n",
    "- hashlib.sha256(data).hexdigest() then equality check.\n",
    "\n",
    "**Why it matters**\n",
    "- Simple data integrity check to detect corrupted downloads or tampering.\n",
    "- Used by other functions to decide whether to trust cached files or raise an error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02aa28fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_hash(data: bytes, expected_hash: str) -> bool:\n",
    "    actual_hash = hashlib.sha256(data).hexdigest()\n",
    "    return actual_hash == expected_hash"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b853467f",
   "metadata": {},
   "source": [
    "**What it does**\n",
    "- Fetches blobpath but uses a local cache directory to avoid re-downloading; validates cache with expected_hash if provided; writes fetched results into the cache (if writable).\n",
    "\n",
    "**How it works (key steps)**\n",
    "1. Determine cache_dir from environment variables TIKTOKEN_CACHE_DIR or DATA_GYM_CACHE_DIR, otherwise use a temp-directory data-gym-cache (and track whether user explicitly set it).\n",
    "2.\tIf cache_dir == \"\", disables caching and calls read_file.\n",
    "3.\tCompute a cache key as SHA-1 of blobpath, check cache_path.\n",
    "4.\tIf cached file exists, read it and return it if expected_hash is None or matches; otherwise delete the cache entry and re-fetch.\n",
    "5.\tIf not cached or cache invalid: call read_file(blobpath) to fetch contents. If expected_hash provided and the fetched content fails the hash, raise ValueError.\n",
    "6.\tAttempt safe atomic write: write to a tmp file then os.rename to cache_path. If the write fails and the cache directory was user-specified, re-raise; if it was the default temp dir, silently ignore write failures.\n",
    "\n",
    "**Why it matters**\n",
    "- Avoids repeated expensive network reads (saves bandwidth/time).\n",
    "- Ensures cached files are trusted via hash validation (prevents using corrupted or tampered cached data).\n",
    "- Uses safe write pattern (tmp file + rename) to avoid partial/half-written cache files.\n",
    "- Respects user opt-out (empty cache path) and tolerates unwritable default caches but fails loudly for user-specified caches (good UX)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31b499b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file_cached(blobpath: str, expected_hash: str | None = None) -> bytes:\n",
    "    user_specified_cache = True\n",
    "    if \"TIKTOKEN_CACHE_DIR\" in os.environ:\n",
    "        cache_dir = os.environ[\"TIKTOKEN_CACHE_DIR\"]\n",
    "    elif \"DATA_GYM_CACHE_DIR\" in os.environ:\n",
    "        cache_dir = os.environ[\"DATA_GYM_CACHE_DIR\"]\n",
    "    else:\n",
    "        import tempfile\n",
    "\n",
    "        cache_dir = os.path.join(tempfile.gettempdir(), \"data-gym-cache\")\n",
    "        user_specified_cache = False\n",
    "\n",
    "    if cache_dir == \"\":\n",
    "        # disable caching\n",
    "        return read_file(blobpath)\n",
    "\n",
    "    cache_key = hashlib.sha1(blobpath.encode()).hexdigest()\n",
    "\n",
    "    cache_path = os.path.join(cache_dir, cache_key)\n",
    "    if os.path.exists(cache_path):\n",
    "        with open(cache_path, \"rb\", buffering=0) as f:\n",
    "            data = f.read()\n",
    "        if expected_hash is None or check_hash(data, expected_hash):\n",
    "            return data\n",
    "\n",
    "        # the cached file does not match the hash, remove it and re-fetch\n",
    "        try:\n",
    "            os.remove(cache_path)\n",
    "        except OSError:\n",
    "            pass\n",
    "\n",
    "    contents = read_file(blobpath)\n",
    "    if expected_hash and not check_hash(contents, expected_hash):\n",
    "        raise ValueError(\n",
    "            f\"Hash mismatch for data downloaded from {blobpath} (expected {expected_hash}). \"\n",
    "            f\"This may indicate a corrupted download. Please try again.\"\n",
    "        )\n",
    "\n",
    "    import uuid\n",
    "\n",
    "    try:\n",
    "        os.makedirs(cache_dir, exist_ok=True)\n",
    "        tmp_filename = cache_path + \".\" + str(uuid.uuid4()) + \".tmp\"\n",
    "        with open(tmp_filename, \"wb\") as f:\n",
    "            f.write(contents)\n",
    "        os.rename(tmp_filename, cache_path)\n",
    "    except OSError:\n",
    "        # don't raise if we can't write to the default cache, e.g. issue #75\n",
    "        if user_specified_cache:\n",
    "            raise\n",
    "\n",
    "    return contents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85d1e2f1",
   "metadata": {},
   "source": [
    "**What it does**\n",
    "- Converts a DataGym style BPE merges file plus encoder JSON into a single mapping token_bytes -> rank that matches the ordering/format tiktoken expects for mergeable BPE ranks.\n",
    "\n",
    "**How it works (overview)**\n",
    "- Builds a mapping of printable single-byte tokens in a deterministic rank order and then appends the other byte values so there are 256 single-byte tokens. (The data_gym_byte_to_byte / rank_to_intbyte logic does this remapping.)\n",
    "- Reads vocab_bpe_file (cached) and parses merge pairs (these describe how two tokens merge to a new token, with ordering/ranks).\n",
    "- Constructs initial bpe_ranks for single-byte tokens, then assigns ranks for merged tokens according to the merges file (in increasing n).\n",
    "- Loads encoder_json_file (cached), decodes keys into bytes, removes special non-mergeable tokens (e.g. <|endoftext|>), and optionally clobbers single-byte tokens with values from encoder JSON if clobber_one_byte_tokens is True.\n",
    "- Asserts that the constructed bpe_ranks equals the encoder JSON mapping ‚Äî this is an important sanity check guaranteeing both inputs are consistent.\n",
    "\n",
    "**Why it matters**\n",
    "- Tokenizers (especially tiktoken-like BPE) rely on a specific mapping from token bytes to ranks where rank order equals merge priority. Converting between different tokenizer repo formats is essential to reuse models, vocabularies, or tooling.\n",
    "- The function enforces consistency between merges and encoder definitions ‚Äî catching mismatched files early prevents subtle tokenization bugs that can silently change model inputs/outputs.\n",
    "- The clobber_one_byte_tokens flag gives controlled flexibility when encoders provide a particular ordering for single-byte tokens.\n",
    "\n",
    "**Practical use**\n",
    "- If you have a merges file and an encoder JSON from another tokenizer/tooling (DataGym), this function produces the exact rank table tiktoken expects so you can instantiate or interoperate with tiktoken-style tokenizers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "510efe2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_gym_to_mergeable_bpe_ranks(\n",
    "    vocab_bpe_file: str,\n",
    "    encoder_json_file: str,\n",
    "    vocab_bpe_hash: str | None = None,\n",
    "    encoder_json_hash: str | None = None,\n",
    "    clobber_one_byte_tokens: bool = False,\n",
    ") -> dict[bytes, int]:\n",
    "    # NB: do not add caching to this function\n",
    "    rank_to_intbyte = [b for b in range(2**8) if chr(b).isprintable() and chr(b) != \" \"]\n",
    "\n",
    "    data_gym_byte_to_byte = {chr(b): b for b in rank_to_intbyte}\n",
    "    n = 0\n",
    "    for b in range(2**8):\n",
    "        if b not in rank_to_intbyte:\n",
    "            rank_to_intbyte.append(b)\n",
    "            data_gym_byte_to_byte[chr(2**8 + n)] = b\n",
    "            n += 1\n",
    "    assert len(rank_to_intbyte) == 2**8\n",
    "\n",
    "    # vocab_bpe contains the merges along with associated ranks\n",
    "    vocab_bpe_contents = read_file_cached(vocab_bpe_file, vocab_bpe_hash).decode()\n",
    "    bpe_merges = [tuple(merge_str.split()) for merge_str in vocab_bpe_contents.split(\"\\n\")[1:-1]]\n",
    "\n",
    "    def decode_data_gym(value: str) -> bytes:\n",
    "        return bytes(data_gym_byte_to_byte[b] for b in value)\n",
    "\n",
    "    # add the single byte tokens\n",
    "    # if clobber_one_byte_tokens is True, we'll replace these with ones from the encoder json\n",
    "    bpe_ranks = {bytes([b]): i for i, b in enumerate(rank_to_intbyte)}\n",
    "    del rank_to_intbyte\n",
    "\n",
    "    # add the merged tokens\n",
    "    n = len(bpe_ranks)\n",
    "    for first, second in bpe_merges:\n",
    "        bpe_ranks[decode_data_gym(first) + decode_data_gym(second)] = n\n",
    "        n += 1\n",
    "\n",
    "    import json\n",
    "\n",
    "    # check that the encoder file matches the merges file\n",
    "    # this sanity check is important since tiktoken assumes that ranks are ordered the same\n",
    "    # as merge priority\n",
    "    encoder_json = json.loads(read_file_cached(encoder_json_file, encoder_json_hash))\n",
    "    encoder_json_loaded = {decode_data_gym(k): v for k, v in encoder_json.items()}\n",
    "    # drop these two special tokens if present, since they're not mergeable bpe tokens\n",
    "    encoder_json_loaded.pop(b\"<|endoftext|>\", None)\n",
    "    encoder_json_loaded.pop(b\"<|startoftext|>\", None)\n",
    "\n",
    "    if clobber_one_byte_tokens:\n",
    "        for k in encoder_json_loaded:\n",
    "            if len(k) == 1:\n",
    "                bpe_ranks[k] = encoder_json_loaded[k]\n",
    "\n",
    "    assert bpe_ranks == encoder_json_loaded\n",
    "\n",
    "    return bpe_ranks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36342c3a",
   "metadata": {},
   "source": [
    "**What it does**\n",
    "- Writes bpe_ranks to tiktoken_bpe_file in a simple text format: each line base64(token) rank\\n, ordered by ascending rank.\n",
    "\n",
    "**How it works**\n",
    "- Ensures blobfile is installed, opens blobfile.BlobFile(..., \"wb\"), sorts bpe_ranks by rank, and writes base64.b64encode(token) + b\" \" + str(rank).encode() + b\"\\n\" for each.\n",
    "\n",
    "**Why it matters**\n",
    "- Produces a portable serialized BPE rank file that other code (including load_tiktoken_bpe or other tools) can read.\n",
    "- Using base64 ensures arbitrary byte tokens are safely stored in a text-like file, avoiding issues with binary data in text files.\n",
    "- Writing via blobfile enables writing to cloud-backed paths as well as local disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c01f052",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dump_tiktoken_bpe(bpe_ranks: dict[bytes, int], tiktoken_bpe_file: str) -> None:\n",
    "    try:\n",
    "        import blobfile\n",
    "    except ImportError as e:\n",
    "        raise ImportError(\n",
    "            \"blobfile is not installed. Please install it by running `pip install blobfile`.\"\n",
    "        ) from e\n",
    "    with blobfile.BlobFile(tiktoken_bpe_file, \"wb\") as f:\n",
    "        for token, rank in sorted(bpe_ranks.items(), key=lambda x: x[1]):\n",
    "            f.write(base64.b64encode(token) + b\" \" + str(rank).encode() + b\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0deb9ada",
   "metadata": {},
   "source": [
    "**What it does**\n",
    "- Reads the file format produced by dump_tiktoken_bpe and returns dict[bytes, int] mapping token bytes to rank (int). Optionally validates via expected_hash on the raw file.\n",
    "\n",
    "**How it works**\n",
    "- Calls read_file_cached with optional expected_hash to get the file contents.\n",
    "- Splits into lines and for each non-empty line splits the token and rank. Decodes the base64 token and converts the rank to int. On parse failure raises a ValueError with context.\n",
    "\n",
    "**Why it matters**\n",
    "- Loads a stable, portable BPE rank table into memory for tokenizer construction/inspection.\n",
    "- Validates format and surfaces parse errors clearly (helps debug mismatched versions or corrupted files).\n",
    "- Works with cached files and supports cloud paths (via read_file_cached internals)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10ef69a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_tiktoken_bpe(tiktoken_bpe_file: str, expected_hash: str | None = None) -> dict[bytes, int]:\n",
    "    # NB: do not add caching to this function\n",
    "    contents = read_file_cached(tiktoken_bpe_file, expected_hash)\n",
    "    ret = {}\n",
    "    for line in contents.splitlines():\n",
    "        if not line:\n",
    "            continue\n",
    "        try:\n",
    "            token, rank = line.split()\n",
    "            ret[base64.b64decode(token)] = int(rank)\n",
    "        except Exception as e:\n",
    "            raise ValueError(f\"Error parsing line {line!r} in {tiktoken_bpe_file}\") from e\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee114431",
   "metadata": {},
   "source": [
    "## **tiktoken/tiktoken/<span style='color:black'> **model.py** </span>**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "793146d6",
   "metadata": {},
   "source": [
    "This module maps model names (or model-name prefixes) to the correct tokenizer/encoding and provides helpers to return the encoding name or the Encoding object for a given model. Select the correct tokenizer/encoding for a model name (including versioned names) so text is tokenized correctly.\n",
    "\n",
    "Tokenization must match the model‚Äôs expected encoding. Using the wrong encoding produces wrong token counts, broken prompt handling, misaligned special tokens, wrong truncation, and can break generation or billing/usage calculations. This module centralizes and automates that mapping so callers can reliably obtain the exact encoding the model expects ‚Äî including for versioned model names ‚Äî without needing manual lookup every time.\n",
    "\n",
    "Maps model names (including versioned names via prefixes) to the exact tokenizer encoding and returns the Encoding object ‚Äî ensuring text is tokenized the way the model expects.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7695dd67",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "from .core import Encoding\n",
    "from .registry import get_encoding\n",
    "\n",
    "# TODO: these will likely be replaced by an API endpoint\n",
    "MODEL_PREFIX_TO_ENCODING: dict[str, str] = {\n",
    "    \"o1-\": \"o200k_base\",\n",
    "    \"o3-\": \"o200k_base\",\n",
    "    \"o4-mini-\": \"o200k_base\",\n",
    "    # chat\n",
    "    \"gpt-5-\": \"o200k_base\",\n",
    "    \"gpt-4.5-\": \"o200k_base\",\n",
    "    \"gpt-4.1-\": \"o200k_base\",\n",
    "    \"chatgpt-4o-\": \"o200k_base\",\n",
    "    \"gpt-4o-\": \"o200k_base\",  # e.g., gpt-4o-2024-05-13\n",
    "    \"gpt-4-\": \"cl100k_base\",  # e.g., gpt-4-0314, etc., plus gpt-4-32k\n",
    "    \"gpt-3.5-turbo-\": \"cl100k_base\",  # e.g, gpt-3.5-turbo-0301, -0401, etc.\n",
    "    \"gpt-35-turbo-\": \"cl100k_base\",  # Azure deployment name\n",
    "    \"gpt-oss-\": \"o200k_harmony\",\n",
    "    # fine-tuned\n",
    "    \"ft:gpt-4o\": \"o200k_base\",\n",
    "    \"ft:gpt-4\": \"cl100k_base\",\n",
    "    \"ft:gpt-3.5-turbo\": \"cl100k_base\",\n",
    "    \"ft:davinci-002\": \"cl100k_base\",\n",
    "    \"ft:babbage-002\": \"cl100k_base\",\n",
    "}\n",
    "\n",
    "MODEL_TO_ENCODING: dict[str, str] = {\n",
    "    # reasoning\n",
    "    \"o1\": \"o200k_base\",\n",
    "    \"o3\": \"o200k_base\",\n",
    "    \"o4-mini\": \"o200k_base\",\n",
    "    # chat\n",
    "    \"gpt-5\": \"o200k_base\",\n",
    "    \"gpt-4.1\": \"o200k_base\",\n",
    "    \"gpt-4o\": \"o200k_base\",\n",
    "    \"gpt-4\": \"cl100k_base\",\n",
    "    \"gpt-3.5-turbo\": \"cl100k_base\",\n",
    "    \"gpt-3.5\": \"cl100k_base\",  # Common shorthand\n",
    "    \"gpt-35-turbo\": \"cl100k_base\",  # Azure deployment name\n",
    "    # base\n",
    "    \"davinci-002\": \"cl100k_base\",\n",
    "    \"babbage-002\": \"cl100k_base\",\n",
    "    # embeddings\n",
    "    \"text-embedding-ada-002\": \"cl100k_base\",\n",
    "    \"text-embedding-3-small\": \"cl100k_base\",\n",
    "    \"text-embedding-3-large\": \"cl100k_base\",\n",
    "    # DEPRECATED MODELS\n",
    "    # text (DEPRECATED)\n",
    "    \"text-davinci-003\": \"p50k_base\",\n",
    "    \"text-davinci-002\": \"p50k_base\",\n",
    "    \"text-davinci-001\": \"r50k_base\",\n",
    "    \"text-curie-001\": \"r50k_base\",\n",
    "    \"text-babbage-001\": \"r50k_base\",\n",
    "    \"text-ada-001\": \"r50k_base\",\n",
    "    \"davinci\": \"r50k_base\",\n",
    "    \"curie\": \"r50k_base\",\n",
    "    \"babbage\": \"r50k_base\",\n",
    "    \"ada\": \"r50k_base\",\n",
    "    # code (DEPRECATED)\n",
    "    \"code-davinci-002\": \"p50k_base\",\n",
    "    \"code-davinci-001\": \"p50k_base\",\n",
    "    \"code-cushman-002\": \"p50k_base\",\n",
    "    \"code-cushman-001\": \"p50k_base\",\n",
    "    \"davinci-codex\": \"p50k_base\",\n",
    "    \"cushman-codex\": \"p50k_base\",\n",
    "    # edit (DEPRECATED)\n",
    "    \"text-davinci-edit-001\": \"p50k_edit\",\n",
    "    \"code-davinci-edit-001\": \"p50k_edit\",\n",
    "    # old embeddings (DEPRECATED)\n",
    "    \"text-similarity-davinci-001\": \"r50k_base\",\n",
    "    \"text-similarity-curie-001\": \"r50k_base\",\n",
    "    \"text-similarity-babbage-001\": \"r50k_base\",\n",
    "    \"text-similarity-ada-001\": \"r50k_base\",\n",
    "    \"text-search-davinci-doc-001\": \"r50k_base\",\n",
    "    \"text-search-curie-doc-001\": \"r50k_base\",\n",
    "    \"text-search-babbage-doc-001\": \"r50k_base\",\n",
    "    \"text-search-ada-doc-001\": \"r50k_base\",\n",
    "    \"code-search-babbage-code-001\": \"r50k_base\",\n",
    "    \"code-search-ada-code-001\": \"r50k_base\",\n",
    "    # open source\n",
    "    \"gpt2\": \"gpt2\",\n",
    "    \"gpt-2\": \"gpt2\",  # Maintains consistency with gpt-4\n",
    "}\n",
    "\n",
    "\n",
    "def encoding_name_for_model(model_name: str) -> str:\n",
    "    \"\"\"Returns the name of the encoding used by a model.\n",
    "\n",
    "    Raises a KeyError if the model name is not recognised.\n",
    "    \"\"\"\n",
    "    encoding_name = None\n",
    "    if model_name in MODEL_TO_ENCODING:\n",
    "        encoding_name = MODEL_TO_ENCODING[model_name]\n",
    "    else:\n",
    "        # Check if the model matches a known prefix\n",
    "        # Prefix matching avoids needing library updates for every model version release\n",
    "        # Note that this can match on non-existent models (e.g., gpt-3.5-turbo-FAKE)\n",
    "        for model_prefix, model_encoding_name in MODEL_PREFIX_TO_ENCODING.items():\n",
    "            if model_name.startswith(model_prefix):\n",
    "                return model_encoding_name\n",
    "\n",
    "    if encoding_name is None:\n",
    "        raise KeyError(\n",
    "            f\"Could not automatically map {model_name} to a tokeniser. \"\n",
    "            \"Please use `tiktoken.get_encoding` to explicitly get the tokeniser you expect.\"\n",
    "        ) from None\n",
    "\n",
    "    return encoding_name\n",
    "\n",
    "\n",
    "def encoding_for_model(model_name: str) -> Encoding:\n",
    "    \"\"\"Returns the encoding used by a model.\n",
    "\n",
    "    Raises a KeyError if the model name is not recognised.\n",
    "    \"\"\"\n",
    "    return get_encoding(encoding_name_for_model(model_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03b9071a",
   "metadata": {},
   "source": [
    "## **tiktoken/tiktoken/<span style='color:black'> **py.typed** </span>**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fdd3f70",
   "metadata": {},
   "source": [
    "## **tiktoken/tiktoken/<span style='color:black'> **registry.py** </span>**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1963df64",
   "metadata": {},
   "source": [
    "This module discovers encoding plugins, constructs Encoding instances on demand, caches them, and exposes functions to get or list available encodings in a thread-safe, lazy, and idempotent way.\n",
    "\n",
    "**One-liner**: ‚ÄúDiscover encoding plugins, build and cache Encoding objects on demand, and provide safe lookups for callers.‚Äù\n",
    "\n",
    "**Why this matters**\n",
    "\n",
    "- Keeps the core tokenizer library extensible: third-party or optional encoding implementations can be packaged as plugins (under tiktoken_ext) and automatically discovered without modifying core code.\n",
    "- Saves work and memory by constructing each Encoding only once (lazy instantiation + caching).\n",
    "- Safe in multi-threaded programs thanks to locking and double-checked patterns ‚Äî avoids race conditions and duplicate construction.\n",
    "- Provides clear, helpful errors when plugins are missing, duplicated, or malformed so users can diagnose install/config problems quickly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6045e3f6",
   "metadata": {},
   "source": [
    "**Important design choices & edge cases**\n",
    "\n",
    "- Lazy + cached discovery and construction: avoids importing all plugins or building every Encoding at startup; constructs only what is used.\n",
    "- Double-checked locking: reduces lock contention for the common fast path (ENCODINGS cache hit) while remaining safe when initializing resources.\n",
    "- Idempotent failure handling: if plugin discovery fails, ENCODING_CONSTRUCTORS is reset to None and the exception is propagated. This avoids leaving a half-initialized broken state and allows retries.\n",
    "- Duplicate name detection: prevents two plugins from claiming the same encoding name, which would create ambiguous behavior at runtime.\n",
    "- Helpful errors: get_encoding gives the list of found plugins and tiktoken version to help debugging installation/version mismatches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cece9bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import functools\n",
    "import importlib\n",
    "import pkgutil\n",
    "import threading\n",
    "from typing import Any, Callable, Sequence\n",
    "\n",
    "import tiktoken_ext\n",
    "\n",
    "import tiktoken\n",
    "from tiktoken.core import Encoding\n",
    "\n",
    "_lock = threading.RLock()\n",
    "ENCODINGS: dict[str, Encoding] = {}\n",
    "ENCODING_CONSTRUCTORS: dict[str, Callable[[], dict[str, Any]]] | None = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef3c97ea",
   "metadata": {},
   "source": [
    "**What it does**\n",
    "- Uses pkgutil.iter_modules over tiktoken_ext.__path__ to list submodule names inside the tiktoken_ext namespace package.\n",
    "- Returns a list of full module names (e.g. tiktoken_ext.some_plugin).\n",
    "\n",
    "**Why it matters**\n",
    "- Fast, safe discovery of installed plugin modules without importing all of them immediately.\n",
    "- Keeps plugin discovery efficient (namespace packages + pkgutil are lightweight and work well with editable installs).\n",
    "\n",
    "**Notes**\n",
    "- Decorated with functools.lru_cache, so it runs discovery once and caches the list for repeated calls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e7e812d",
   "metadata": {},
   "outputs": [],
   "source": [
    "@functools.lru_cache\n",
    "def _available_plugin_modules() -> Sequence[str]:\n",
    "    # tiktoken_ext is a namespace package\n",
    "    # submodules inside tiktoken_ext will be inspected for ENCODING_CONSTRUCTORS attributes\n",
    "    # - we use namespace package pattern so `pkgutil.iter_modules` is fast\n",
    "    # - it's a separate top-level package because namespace subpackages of non-namespace\n",
    "    #   packages don't quite do what you want with editable installs\n",
    "    mods = []\n",
    "    plugin_mods = pkgutil.iter_modules(tiktoken_ext.__path__, tiktoken_ext.__name__ + \".\")\n",
    "    for _, mod_name, _ in plugin_mods:\n",
    "        mods.append(mod_name)\n",
    "    return mods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "395f7e4f",
   "metadata": {},
   "source": [
    "**What it does**\n",
    "- Thread-safely initializes ENCODING_CONSTRUCTORS if not already initialized.\n",
    "- Iterates all plugin modules from _available_plugin_modules().\n",
    "- Imports each plugin module and expects the module to define ENCODING_CONSTRUCTORS.\n",
    "- Validates no duplicate encoding names across plugins; stores constructors in ENCODING_CONSTRUCTORS.\n",
    "- On any exception during discovery, resets ENCODING_CONSTRUCTORS to None and re-raises (idempotent error behavior).\n",
    "\n",
    "**Why it matters**\n",
    "- Centralizes and validates plugin discovery logic.\n",
    "- Prevents ambiguous constructors (duplicate names) which would cause unpredictable behavior later.\n",
    "- The idempotent failure (reset to None) ensures subsequent calls can retry discovery rather than remaining in a broken partial state.\n",
    "\n",
    "**Concurrency pattern**\n",
    "- Uses _lock and a check-then-act approach (double-checked locking) so only one thread performs discovery and others wait/see results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27c4f2e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def _find_constructors() -> None:\n",
    "    global ENCODING_CONSTRUCTORS\n",
    "    with _lock:\n",
    "        if ENCODING_CONSTRUCTORS is not None:\n",
    "            return\n",
    "        ENCODING_CONSTRUCTORS = {}\n",
    "\n",
    "        try:\n",
    "            for mod_name in _available_plugin_modules():\n",
    "                mod = importlib.import_module(mod_name)\n",
    "                try:\n",
    "                    constructors = mod.ENCODING_CONSTRUCTORS\n",
    "                except AttributeError as e:\n",
    "                    raise ValueError(\n",
    "                        f\"tiktoken plugin {mod_name} does not define ENCODING_CONSTRUCTORS\"\n",
    "                    ) from e\n",
    "                for enc_name, constructor in constructors.items():\n",
    "                    if enc_name in ENCODING_CONSTRUCTORS:\n",
    "                        raise ValueError(\n",
    "                            f\"Duplicate encoding name {enc_name} in tiktoken plugin {mod_name}\"\n",
    "                        )\n",
    "                    ENCODING_CONSTRUCTORS[enc_name] = constructor\n",
    "        except Exception:\n",
    "            # Ensure we idempotently raise errors\n",
    "            ENCODING_CONSTRUCTORS = None\n",
    "            raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21f9e2b9",
   "metadata": {},
   "source": [
    "**What it does**\n",
    "1. Validates encoding_name is a str.\n",
    "2.\tChecks ENCODINGS cache ‚Äî if present, returns immediately (fast path, no lock).\n",
    "3.\tAcquires _lock and re-checks cache (avoid race).\n",
    "4.\tIf constructors are not yet discovered, calls _find_constructors().\n",
    "5.\tLooks up encoding_name in ENCODING_CONSTRUCTORS. If not found raises ValueError with helpful diagnostics (which plugins were found and what tiktoken version is installed).\n",
    "6.\tCalls the constructor to get kwargs, constructs Encoding(**kwargs), caches it in ENCODINGS, and returns it.\n",
    "\n",
    "**Why it matters**\n",
    "- The main public accessor for getting a usable Encoding. Consumers call this to get an object with .encode()/.decode() etc.\n",
    "- Combines lazy discovery + lazy construction + caching for efficiency.\n",
    "- Provides clear errors when an encoding isn‚Äôt available or when plugin/config issues exist.\n",
    "- Thread-safe: two parallel requests for the same encoding won‚Äôt construct two different Encoding instances.\n",
    "\n",
    "**Behavioral guarantees**\n",
    "- Returns the same Encoding instance for repeated calls with the same name (identity preserved).\n",
    "- Raises early and informatively when input is wrong type or encoding unavailable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1ea8277",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_encoding(encoding_name: str) -> Encoding:\n",
    "    if not isinstance(encoding_name, str):\n",
    "        raise ValueError(f\"Expected a string in get_encoding, got {type(encoding_name)}\")\n",
    "\n",
    "    if encoding_name in ENCODINGS:\n",
    "        return ENCODINGS[encoding_name]\n",
    "\n",
    "    with _lock:\n",
    "        if encoding_name in ENCODINGS:\n",
    "            return ENCODINGS[encoding_name]\n",
    "\n",
    "        if ENCODING_CONSTRUCTORS is None:\n",
    "            _find_constructors()\n",
    "            assert ENCODING_CONSTRUCTORS is not None\n",
    "\n",
    "        if encoding_name not in ENCODING_CONSTRUCTORS:\n",
    "            raise ValueError(\n",
    "                f\"Unknown encoding {encoding_name}.\\n\"\n",
    "                f\"Plugins found: {_available_plugin_modules()}\\n\"\n",
    "                f\"tiktoken version: {tiktoken.__version__} (are you on latest?)\"\n",
    "            )\n",
    "\n",
    "        constructor = ENCODING_CONSTRUCTORS[encoding_name]\n",
    "        enc = Encoding(**constructor())\n",
    "        ENCODINGS[encoding_name] = enc\n",
    "        return enc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caa6579d",
   "metadata": {},
   "source": [
    "**What it does**\n",
    "- Ensures constructors are discovered (via _find_constructors()), then returns a list of available encoding names (keys of ENCODING_CONSTRUCTORS).\n",
    "\n",
    "**Why it matters**\n",
    "- Provides a programmatic way to inspect which encodings are available (helpful for diagnostics, autocompletion, or UI lists).\n",
    "- Respects lazy discovery: if discovery hasn‚Äôt run yet it will run now and populate constructors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0178265",
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_encoding_names() -> list[str]:\n",
    "    with _lock:\n",
    "        if ENCODING_CONSTRUCTORS is None:\n",
    "            _find_constructors()\n",
    "            assert ENCODING_CONSTRUCTORS is not None\n",
    "        return list(ENCODING_CONSTRUCTORS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a8779dd",
   "metadata": {},
   "source": [
    "# **tiktoken/<span style='color: #0e90b1ff;'> **tiktoken_ext** </span>**\n",
    "\n",
    "**Overall responsibility (short)**\n",
    "\n",
    "- This module declares several named token-encoding constructors (**GPT-2, r50k, p50k, cl100k, o200k** variants) ‚Äî each returns the data (pat_str, BPE ranks / mergeable ranks, special token id map, vocab size hint) needed to create a tokenizer compatible with a particular model/encoding.\n",
    "\n",
    "**Why that matters**\n",
    "\n",
    "- Tokenization is the input interface to language models. Precise regexes, BPE merge ranks, and reserved token IDs ensure deterministic encoding/decoding, cross-implementation compatibility, correct model inputs, and security (hash checks). Without these configurations models would get different token streams and produce incorrect or irreproducible outputs.\n",
    "\n",
    "**one-liner summary**\n",
    "\n",
    "- A registry of ready-to-use tokenizer configurations (regex patterns, BPE ranks, and special token mappings) for multiple model encodings so tiktoken can build deterministic, model-compatible tokenizers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e5f45f49",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tiktoken.load import data_gym_to_mergeable_bpe_ranks, load_tiktoken_bpe\n",
    "\n",
    "ENDOFTEXT = \"<|endoftext|>\"\n",
    "FIM_PREFIX = \"<|fim_prefix|>\"\n",
    "FIM_MIDDLE = \"<|fim_middle|>\"\n",
    "FIM_SUFFIX = \"<|fim_suffix|>\"\n",
    "ENDOFPROMPT = \"<|endofprompt|>\"\n",
    "\n",
    "# The pattern in the original GPT-2 release is:\n",
    "# r\"\"\"'s|'t|'re|'ve|'m|'ll|'d| ?[\\p{L}]+| ?[\\p{N}]+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+\"\"\"\n",
    "# This is equivalent, but executes faster:\n",
    "r50k_pat_str = (\n",
    "    r\"\"\"'(?:[sdmt]|ll|ve|re)| ?\\p{L}++| ?\\p{N}++| ?[^\\s\\p{L}\\p{N}]++|\\s++$|\\s+(?!\\S)|\\s\"\"\"\n",
    ")\n",
    "\n",
    "\n",
    "def gpt2():\n",
    "    mergeable_ranks = data_gym_to_mergeable_bpe_ranks(\n",
    "        vocab_bpe_file=\"https://openaipublic.blob.core.windows.net/gpt-2/encodings/main/vocab.bpe\",\n",
    "        encoder_json_file=\"https://openaipublic.blob.core.windows.net/gpt-2/encodings/main/encoder.json\",\n",
    "        vocab_bpe_hash=\"1ce1664773c50f3e0cc8842619a93edc4624525b728b188a9e0be33b7726adc5\",\n",
    "        encoder_json_hash=\"196139668be63f3b5d6574427317ae82f612a97c5d1cdaf36ed2256dbf636783\",\n",
    "    )\n",
    "    return {\n",
    "        \"name\": \"gpt2\",\n",
    "        \"explicit_n_vocab\": 50257,\n",
    "        \"pat_str\": r50k_pat_str,\n",
    "        \"mergeable_ranks\": mergeable_ranks,\n",
    "        \"special_tokens\": {ENDOFTEXT: 50256},\n",
    "    }\n",
    "\n",
    "\n",
    "def r50k_base():\n",
    "    mergeable_ranks = load_tiktoken_bpe(\n",
    "        \"https://openaipublic.blob.core.windows.net/encodings/r50k_base.tiktoken\",\n",
    "        expected_hash=\"306cd27f03c1a714eca7108e03d66b7dc042abe8c258b44c199a7ed9838dd930\",\n",
    "    )\n",
    "    return {\n",
    "        \"name\": \"r50k_base\",\n",
    "        \"explicit_n_vocab\": 50257,\n",
    "        \"pat_str\": r50k_pat_str,\n",
    "        \"mergeable_ranks\": mergeable_ranks,\n",
    "        \"special_tokens\": {ENDOFTEXT: 50256},\n",
    "    }\n",
    "\n",
    "\n",
    "def p50k_base():\n",
    "    mergeable_ranks = load_tiktoken_bpe(\n",
    "        \"https://openaipublic.blob.core.windows.net/encodings/p50k_base.tiktoken\",\n",
    "        expected_hash=\"94b5ca7dff4d00767bc256fdd1b27e5b17361d7b8a5f968547f9f23eb70d2069\",\n",
    "    )\n",
    "    return {\n",
    "        \"name\": \"p50k_base\",\n",
    "        \"explicit_n_vocab\": 50281,\n",
    "        \"pat_str\": r50k_pat_str,\n",
    "        \"mergeable_ranks\": mergeable_ranks,\n",
    "        \"special_tokens\": {ENDOFTEXT: 50256},\n",
    "    }\n",
    "\n",
    "\n",
    "def p50k_edit():\n",
    "    mergeable_ranks = load_tiktoken_bpe(\n",
    "        \"https://openaipublic.blob.core.windows.net/encodings/p50k_base.tiktoken\",\n",
    "        expected_hash=\"94b5ca7dff4d00767bc256fdd1b27e5b17361d7b8a5f968547f9f23eb70d2069\",\n",
    "    )\n",
    "    special_tokens = {ENDOFTEXT: 50256, FIM_PREFIX: 50281, FIM_MIDDLE: 50282, FIM_SUFFIX: 50283}\n",
    "    return {\n",
    "        \"name\": \"p50k_edit\",\n",
    "        \"pat_str\": r50k_pat_str,\n",
    "        \"mergeable_ranks\": mergeable_ranks,\n",
    "        \"special_tokens\": special_tokens,\n",
    "    }\n",
    "\n",
    "\n",
    "def cl100k_base():\n",
    "    mergeable_ranks = load_tiktoken_bpe(\n",
    "        \"https://openaipublic.blob.core.windows.net/encodings/cl100k_base.tiktoken\",\n",
    "        expected_hash=\"223921b76ee99bde995b7ff738513eef100fb51d18c93597a113bcffe865b2a7\",\n",
    "    )\n",
    "    special_tokens = {\n",
    "        ENDOFTEXT: 100257,\n",
    "        FIM_PREFIX: 100258,\n",
    "        FIM_MIDDLE: 100259,\n",
    "        FIM_SUFFIX: 100260,\n",
    "        ENDOFPROMPT: 100276,\n",
    "    }\n",
    "    return {\n",
    "        \"name\": \"cl100k_base\",\n",
    "        \"pat_str\": r\"\"\"'(?i:[sdmt]|ll|ve|re)|[^\\r\\n\\p{L}\\p{N}]?+\\p{L}++|\\p{N}{1,3}+| ?[^\\s\\p{L}\\p{N}]++[\\r\\n]*+|\\s++$|\\s*[\\r\\n]|\\s+(?!\\S)|\\s\"\"\",\n",
    "        \"mergeable_ranks\": mergeable_ranks,\n",
    "        \"special_tokens\": special_tokens,\n",
    "    }\n",
    "\n",
    "\n",
    "def o200k_base():\n",
    "    mergeable_ranks = load_tiktoken_bpe(\n",
    "        \"https://openaipublic.blob.core.windows.net/encodings/o200k_base.tiktoken\",\n",
    "        expected_hash=\"446a9538cb6c348e3516120d7c08b09f57c36495e2acfffe59a5bf8b0cfb1a2d\",\n",
    "    )\n",
    "    special_tokens = {ENDOFTEXT: 199999, ENDOFPROMPT: 200018}\n",
    "    # This regex could be made more efficient. If I was the one working on this encoding, I would\n",
    "    # have done a few other things differently too, e.g. I think you can allocate tokens more\n",
    "    # efficiently across languages.\n",
    "    pat_str = \"|\".join(\n",
    "        [\n",
    "            r\"\"\"[^\\r\\n\\p{L}\\p{N}]?[\\p{Lu}\\p{Lt}\\p{Lm}\\p{Lo}\\p{M}]*[\\p{Ll}\\p{Lm}\\p{Lo}\\p{M}]+(?i:'s|'t|'re|'ve|'m|'ll|'d)?\"\"\",\n",
    "            r\"\"\"[^\\r\\n\\p{L}\\p{N}]?[\\p{Lu}\\p{Lt}\\p{Lm}\\p{Lo}\\p{M}]+[\\p{Ll}\\p{Lm}\\p{Lo}\\p{M}]*(?i:'s|'t|'re|'ve|'m|'ll|'d)?\"\"\",\n",
    "            r\"\"\"\\p{N}{1,3}\"\"\",\n",
    "            r\"\"\" ?[^\\s\\p{L}\\p{N}]+[\\r\\n/]*\"\"\",\n",
    "            r\"\"\"\\s*[\\r\\n]+\"\"\",\n",
    "            r\"\"\"\\s+(?!\\S)\"\"\",\n",
    "            r\"\"\"\\s+\"\"\",\n",
    "        ]\n",
    "    )\n",
    "    return {\n",
    "        \"name\": \"o200k_base\",\n",
    "        \"pat_str\": pat_str,\n",
    "        \"mergeable_ranks\": mergeable_ranks,\n",
    "        \"special_tokens\": special_tokens,\n",
    "    }\n",
    "\n",
    "\n",
    "def o200k_harmony():\n",
    "    base_enc = o200k_base()\n",
    "    name = \"o200k_harmony\"\n",
    "    pat_str = base_enc[\"pat_str\"]\n",
    "    mergeable_ranks = base_enc[\"mergeable_ranks\"]\n",
    "    special_tokens = {\n",
    "        **base_enc[\"special_tokens\"],\n",
    "        \"<|startoftext|>\": 199998,\n",
    "        \"<|endoftext|>\": 199999,\n",
    "        \"<|reserved_200000|>\": 200000,\n",
    "        \"<|reserved_200001|>\": 200001,\n",
    "        \"<|return|>\": 200002,\n",
    "        \"<|constrain|>\": 200003,\n",
    "        \"<|reserved_200004|>\": 200004,\n",
    "        \"<|channel|>\": 200005,\n",
    "        \"<|start|>\": 200006,\n",
    "        \"<|end|>\": 200007,\n",
    "        \"<|message|>\": 200008,\n",
    "        \"<|reserved_200009|>\": 200009,\n",
    "        \"<|reserved_200010|>\": 200010,\n",
    "        \"<|reserved_200011|>\": 200011,\n",
    "        \"<|call|>\": 200012,\n",
    "    } | {f\"<|reserved_{i}|>\": i for i in range(200013, 201088)}\n",
    "    return {\n",
    "        \"name\": name,\n",
    "        \"pat_str\": pat_str,\n",
    "        \"mergeable_ranks\": mergeable_ranks,\n",
    "        \"special_tokens\": special_tokens,\n",
    "    }\n",
    "\n",
    "\n",
    "ENCODING_CONSTRUCTORS = {\n",
    "    \"gpt2\": gpt2,\n",
    "    \"r50k_base\": r50k_base,\n",
    "    \"p50k_base\": p50k_base,\n",
    "    \"p50k_edit\": p50k_edit,\n",
    "    \"cl100k_base\": cl100k_base,\n",
    "    \"o200k_base\": o200k_base,\n",
    "    \"o200k_harmony\": o200k_harmony,\n",
    "}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
